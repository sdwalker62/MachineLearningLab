{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LongRunTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ITSJA5hWASDn",
        "0JtgL-iQkqj2"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/sdwalker62/e80f2d2e31c8830c286faf9537853e31/longruntransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejXAKpdM7vpW"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dspdnsbX63Z2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f51afada-c141-481e-8754-437a23a3f792"
      },
      "source": [
        "!pip install plotly==4.5\n",
        "import plotly\n",
        "plotly.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: plotly==4.5 in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==4.5) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly==4.5) (1.3.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mkVe1nj2Dn_"
      },
      "source": [
        "%%capture\n",
        "!pip install drain3\n",
        "!pip install jupyter-dash\n",
        "!pip install -Uqq ipdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TIVuGHGicul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4817e9d-e4d2-4f8b-a262-ed60d9df0f94"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZmbFWc97xsl"
      },
      "source": [
        "Feel free to change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLnvgfRy2U1H"
      },
      "source": [
        "# -- Base -- #\n",
        "import os\n",
        "import joblib\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "import io\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import ipdb\n",
        "from copy import deepcopy\n",
        "import sys\n",
        "\n",
        "# -- Metrics -- #\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sqlite3 as sql\n",
        "import tensorboard\n",
        "\n",
        "# -- Tensorflow -- #\n",
        "import tensorflow as tf\n",
        "from jax import jit\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# -- Misc Models -- #\n",
        "import drain3\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# -- Dash -- #\n",
        "from jupyter_dash import JupyterDash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output, State\n",
        "\n",
        "!cp /content/drive/MyDrive/Work/drain3.ini /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31TsP0lroS2f"
      },
      "source": [
        "%pdb on"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L19JQx8YwmH"
      },
      "source": [
        "Extensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM0ON9lGYvGe"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "# Set up logging.\n",
        "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = SOURCE + 'logs/func/%s' % stamp\n",
        "writer = tf.summary.create_file_writer(logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wOAfM3VZIWl"
      },
      "source": [
        "view graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uJdL8IGZKdL"
      },
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/Work/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdlwPqq97n1m"
      },
      "source": [
        "Check if GPU is in use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1kUaw097qmg"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N_5Z7Gg4Jl5"
      },
      "source": [
        "## Environmental Variables\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvZC0zAR4NUe"
      },
      "source": [
        "SOURCE = '/content/drive/MyDrive/Work/'\n",
        "\n",
        "# -- TRANSFORMER Pipeline -- #\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 1\n",
        "DROPOUT_RATE = 0.1\n",
        "MAX_SEQ_LEN = 200\n",
        "\n",
        "ACTIVATION = \"elu\"\n",
        "\n",
        "TRANSFORMER_LAYERS = 4\n",
        "TRANSFORMER_DFF = 2000\n",
        "TRANSFORMER_HEADS = 8\n",
        "\n",
        "TRAINING = True\n",
        "CONTAINER = 'core.soaesb'\n",
        "\n",
        "# -- WORD2VEC Pipeline -- #\n",
        "WINDOW_SIZE = 10\n",
        "GENERATE_NEW_DRAIN = True\n",
        "NUM_NEGATIVE_SAMPLING = 10\n",
        "W2V_BATCH_SIZE = 2048\n",
        "BUFFER_SIZE = 10000\n",
        "W2V_EPOCHS = 200\n",
        "W2V_EMBED_SIZE = 64\n",
        "MAX_VOCAB_SIZE = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8EnuKFhAuER"
      },
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUivYRdyAstr"
      },
      "source": [
        "logging.basicConfig(format='%(asctime)s %(levelname)s | %(message)s',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITSJA5hWASDn"
      },
      "source": [
        "# Define Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnCJ678Ka-T1"
      },
      "source": [
        "## Define Database Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwi7HlrBkwwl"
      },
      "source": [
        "def database_builder(path: str) -> pd.DataFrame():\n",
        "    logger.info('Building DataFrame ...')\n",
        "    (_, _, files) = next(os.walk(path))\n",
        "    sql_query = 'SELECT * FROM logs'\n",
        "    data = []\n",
        "    for f in files:\n",
        "        if '.db' in f:\n",
        "            conn = create_connection(path + f)\n",
        "            d = pd.read_sql_query(sql_query, conn)\n",
        "            data.append(d)\n",
        "    logger.info('...complete!')\n",
        "    return pd.concat(data)\n",
        "\n",
        "\n",
        "def create_connection(path: str) -> sql.Connection:\n",
        "    \"\"\"\n",
        "    Creates a database connection\n",
        "    :param path: str\n",
        "        path to database object\n",
        "    :return sql.Connection\n",
        "        a connection to the database\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sql.connect(path)\n",
        "        logger.info('Connected to database ' + path)\n",
        "        return conn\n",
        "    except sql.Error as e:\n",
        "        logger.warning(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekFXItHibG7r"
      },
      "source": [
        "## Define Dataset Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50ge2uap_kTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af603cfd-5c79-4772-b0d7-9ab271eb4f4f"
      },
      "source": [
        "dataset = database_builder(SOURCE + 'database/')\n",
        "container_dataset = dataset[dataset['container_name']==CONTAINER]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 17:25:43,168 INFO | Building DataFrame ...\n",
            "2021-04-29 17:25:43,191 INFO | Connected to database /content/drive/MyDrive/Work/database/elastic_logs.db\n",
            "2021-04-29 17:25:44,539 INFO | ...complete!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkC2MdmCBR7C"
      },
      "source": [
        "# W2V Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wFAAOw-q3w5"
      },
      "source": [
        "## Pipeline Objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvhQ3bvitv5b"
      },
      "source": [
        "### W2V_Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_nmehscsdmv"
      },
      "source": [
        "class W2V_Pipeline(tf.keras.Model):\n",
        "    def __init__(self, \n",
        "                 save_model,\n",
        "                 load_model):\n",
        "        \n",
        "        super(W2V_Pipeline, self).__init__()\n",
        "        self.save_model = save_model\n",
        "        self.load_model = load_model\n",
        "        \n",
        "        self.PCL = PhraseCaptureLayer(\n",
        "            5, 7, \n",
        "            load_model=load_model, \n",
        "            save_model=save_model)\n",
        "        \n",
        "        self.TCL = TextClusteringLayer(\n",
        "            load_model=load_model, \n",
        "            save_model=save_model)\n",
        "        \n",
        "        self.NSL = NegativeSkipgramLayer(W2V_EMBED_SIZE)\n",
        "\n",
        "        self.W2V = Word2VecEmbeddingLayer(\n",
        "            W2V_EMBED_SIZE, \n",
        "            load_model=load_model, \n",
        "            save_model=save_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = standardize_logs(x)\n",
        "        x = self.PCL(x)\n",
        "        x = self.TCL(x)\n",
        "        x = self.NSL(x)\n",
        "        return self.W2V(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxst8UQeq8sL"
      },
      "source": [
        "### Standardize Logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozo6tpunpywY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIbxj60aLAfC"
      },
      "source": [
        "def standardize_logs(logs: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    # remove timestamps\n",
        "    logs['log'] = logs['log'].replace(\n",
        "        to_replace=r'(?:\\d{4}-\\d{2}-\\d{2}[\\sT]\\d{2}:\\d{2}:\\d{2}([.,]\\d{3}|\\s))|(?:\\s{2,})',\n",
        "        value=' ',\n",
        "        regex=True)\n",
        "    \n",
        "    return logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLKVLZioq_2i"
      },
      "source": [
        "### PhraseCaptureLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFTBqUREEHLn"
      },
      "source": [
        "class PhraseCaptureLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 min_count,\n",
        "                 threshold,\n",
        "                 load_model = False,\n",
        "                 save_model = True):\n",
        "\n",
        "        super(PhraseCaptureLayer, self).__init__()\n",
        "        self.min_count = min_count\n",
        "        self.threshold = threshold\n",
        "        self.load_model = load_model\n",
        "        self.save_model = save_model\n",
        "        \n",
        "        if self.load_model:\n",
        "            self.phrase_model = joblib.load(SOURCE + '/results/phrase_model.joblib')\n",
        "        else:\n",
        "            self.phrase_model = Phrases(min_count=self.min_count, threshold=self.threshold)\n",
        "\n",
        "    def call(self, corpus, training = True):\n",
        "\n",
        "        def clean_log(log):\n",
        "            log = log.lower().strip()\n",
        "            return re.sub(r'\\s{2,}', ' ', log)\n",
        "\n",
        "        def reorganize_return(corpus_with_phrases):\n",
        "            l = []\n",
        "            for tokenized_log in corpus_with_phrases:\n",
        "                l.append(' '.join(tokenized_log))\n",
        "            return l\n",
        "\n",
        "        split_corpus =[log.split(' ') for log in corpus['log']]\n",
        "        \n",
        "        if not training:\n",
        "            self.phrase_model = self.phrase_model.freeze()\n",
        "\n",
        "        self.phrase_model.add_vocab(split_corpus)\n",
        "\n",
        "        if self.save_model:\n",
        "            joblib.dump(self.phrase_model, SOURCE + '/results/phrase_model.joblib')\n",
        "        \n",
        "        corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
        "        return reorganize_return(corpus_with_phrases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb6Ncm7-rKLZ"
      },
      "source": [
        "### TextClusteringLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGOICjK9MtxY"
      },
      "source": [
        "class TextClusteringLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, \n",
        "                 load_model = False,\n",
        "                 save_model = True):\n",
        "        \n",
        "        super(TextClusteringLayer, self).__init__()\n",
        "        self.load_model = load_model\n",
        "        self.save_model = save_model\n",
        "\n",
        "        if load_model:\n",
        "            self.template_miner = joblib.load(SOURCE + '/results/template_miner.joblib')\n",
        "        else:\n",
        "            self.template_miner = drain3.TemplateMiner()\n",
        "\n",
        "    def call(self, corpus, training = True):\n",
        "        if training:\n",
        "            for log in corpus:\n",
        "                self.template_miner.add_log_message(log)\n",
        "            if self.save_model:\n",
        "                joblib.dump(self.template_miner, SOURCE + '/results/template_miner.joblib')\n",
        "            \n",
        "            print(len(self.template_miner.drain.clusters))\n",
        "\n",
        "            return [re.sub(pattern=r' +',\n",
        "                       repl=' ',\n",
        "                       string=cluster.get_template())\n",
        "                    for cluster in self.template_miner.drain.clusters]\n",
        "        else: \n",
        "            l = []\n",
        "            for log in corpus:\n",
        "                match_cluster = self.template_miner.match(log)\n",
        "                if match_cluster is None:\n",
        "                    match_cluster = self.template_miner.add_log_message(log)\n",
        "                l.append(match_cluster)\n",
        "            return [re.sub(pattern=r' +',\n",
        "                       repl=' ',\n",
        "                       string=cluster.get_template())\n",
        "                    for cluster in l]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl_BXs9DrSA5"
      },
      "source": [
        "### NegativeSkipgramLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAfbeuTcnFQK"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class NSLBundle:\n",
        "    vocab: dict\n",
        "    targets: list\n",
        "    contexts: list\n",
        "    labels: list\n",
        "\n",
        "class NegativeSkipgramLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim,\n",
        "                 window_size = 10,\n",
        "                 save_data = True):\n",
        "        \n",
        "        super(NegativeSkipgramLayer, self).__init__()\n",
        "        self.vocab_size = 0\n",
        "        self.vectorized_logs, self.corpus = [], []\n",
        "        self.targets, self.contexts, self.labels = [], [], []\n",
        "        self.vocab = {}\n",
        "        self.window_size = window_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.save_data = save_data\n",
        "\n",
        "    def collect_vocabulary(self):\n",
        "\n",
        "        idx = 1\n",
        "        self.vocab[0] = '<pad>'\n",
        "\n",
        "        log_tokenizer.fit_on_texts(self.corpus)\n",
        "        self.vectorized_logs = log_tokenizer.texts_to_sequences(self.corpus)\n",
        "\n",
        "        self.vocab.update({v: k for k, v in log_tokenizer.word_index.items()})\n",
        "        self.vocab_size = len(self.vocab.keys())\n",
        "\n",
        "    def find_word_context(self):\n",
        "\n",
        "        # Build the sampling table for vocab_size tokens.\n",
        "        sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(len(self.vocab))\n",
        "\n",
        "        for sequence in tqdm(self.vectorized_logs, position=0, leave=True):\n",
        "\n",
        "            positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "                sequence,\n",
        "                vocabulary_size=len(self.vocab),\n",
        "                sampling_table=sampling_table,\n",
        "                window_size=self.window_size,\n",
        "                negative_samples=0)\n",
        "\n",
        "            for target_word, context_word in positive_skip_grams:\n",
        "                context_class = tf.expand_dims(\n",
        "                    tf.constant([context_word], dtype='int64'), 1)\n",
        "\n",
        "                negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "                    true_classes=context_class,\n",
        "                    num_true=1,\n",
        "                    num_sampled=NUM_NEGATIVE_SAMPLING,\n",
        "                    unique=True,\n",
        "                    range_max=len(self.vocab),\n",
        "                    seed=42,\n",
        "                    name=\"negative_sampling\")\n",
        "\n",
        "                negative_sampling_candidates = tf.expand_dims(\n",
        "                    negative_sampling_candidates, 1)\n",
        "\n",
        "                context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "                label = tf.constant([1] + [0] * NUM_NEGATIVE_SAMPLING, dtype='int64')\n",
        "\n",
        "                self.targets.append(target_word)\n",
        "                self.contexts.append(context)\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def call(self, corpus, training = True):\n",
        "\n",
        "        self.corpus = corpus\n",
        "        self.collect_vocabulary()\n",
        "        self.find_word_context()\n",
        "\n",
        "        if self.save_data:\n",
        "            joblib.dump(self.vocab, SOURCE + '/results/vocab.joblib')\n",
        "            joblib.dump(self.targets, SOURCE + '/results/targets.joblib')\n",
        "            joblib.dump(self.contexts, SOURCE + '/results/contexts.joblib')\n",
        "            joblib.dump(self.labels, SOURCE + '/results/labels.joblib')\n",
        "\n",
        "        return NSLBundle(self.vocab, self.targets, self.contexts, self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxXEjBM_rWYC"
      },
      "source": [
        "### Word2VecEmbeddingLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JyKVfDf8CaP"
      },
      "source": [
        "class Word2VecEmbeddingLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, \n",
        "                 embedding_dim,\n",
        "                 load_model = False, \n",
        "                 save_model = True):\n",
        "\n",
        "        super(Word2VecEmbeddingLayer, self).__init__()\n",
        "        self.embeddings = {}\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.load_model = load_model\n",
        "        self.save_model = save_model\n",
        "        self.Optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "        if load_model:\n",
        "            self.Word2Vec = load_model(SOURCE + '/results/word2vec')\n",
        "        else:\n",
        "            self.Word2Vec= None\n",
        "\n",
        "    def call(self, in_bundle, training):\n",
        "\n",
        "        vocab = in_bundle.vocab\n",
        "        targets = in_bundle.targets\n",
        "        contexts = in_bundle.contexts\n",
        "        labels = in_bundle.labels\n",
        "\n",
        "        if not self.load_model and self.Word2Vec is None:\n",
        "            self.Word2Vec = Word2Vec(len(vocab.keys()), self.embedding_dim)\n",
        "            self.Word2Vec.compile(\n",
        "                optimizer=self.Optimizer,\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "        dataset = dataset.shuffle(BUFFER_SIZE).batch(W2V_BATCH_SIZE, drop_remainder=True)\n",
        "        dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        self.Word2Vec.fit(dataset, epochs=W2V_EPOCHS)\n",
        "        weights = self.Word2Vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "\n",
        "        for word in vocab.items():\n",
        "            self.embeddings.update({\n",
        "                word[1]: weights[word[0]]\n",
        "                })\n",
        "\n",
        "        if self.save_model:\n",
        "            self.Word2Vec.save(SOURCE + '/results/word2vec')\n",
        "            out_v = io.open(SOURCE + '/results/vectors.tsv', 'w', encoding='utf-8')\n",
        "            out_m = io.open(SOURCE + '/results/metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "            for index, word in enumerate(vocab.values()):\n",
        "                if index == 0:\n",
        "                    continue  # skip 0, it's padding.\n",
        "                vec = weights[index]\n",
        "                out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "                out_m.write(word + \"\\n\")\n",
        "            out_v.close()\n",
        "            out_m.close()\n",
        "\n",
        "        self.Word2Vec.summary()\n",
        "        return self.embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hQRAbPfrZwt"
      },
      "source": [
        "### Word2VecModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_GiqTup8AY3"
      },
      "source": [
        "class Word2Vec(tf.keras.models.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.target_embedding = tf.keras.layers.Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim,\n",
        "            input_length=1, # input length 1 since we are focusing on one token\n",
        "            name=\"w2v_embedding\")\n",
        "\n",
        "        self.context_embedding = tf.keras.layers.Embedding(\n",
        "            vocab_size,\n",
        "            embedding_dim,\n",
        "            input_length=NUM_NEGATIVE_SAMPLING + 1) # window size for contextual \n",
        "            # reasoning behind the sample token\n",
        "        self.dots = tf.keras.layers.Dot(axes=(3, 2))\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "    def call(self, pair):\n",
        "        target, context = pair\n",
        "        we = self.target_embedding(target)\n",
        "        ce = self.context_embedding(context)\n",
        "        dots = self.dots([ce, we])\n",
        "        return self.flatten(dots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiNfWpckbW3A"
      },
      "source": [
        "## W2V Pipeline Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll4n92B5rGYW"
      },
      "source": [
        "# ** Preprocessing **\n",
        "'''\n",
        "standardize_logs \n",
        "'''\n",
        "\n",
        "# ** Model **\n",
        "# 1. \n",
        "# LogTokenEmbedder\n",
        "'''\n",
        "Seq = [PCL\n",
        "       TCL \n",
        "       NSL\n",
        "       GT1: W2V] -> {embedding_matrix, vocab} \n",
        "'''\n",
        "######\n",
        "\n",
        "# 2.\n",
        "# Transformer Stuff \n",
        "'''\n",
        "{log, embedding_matrix, vocab} ->\n",
        "GT2: Transformer -> prediction \n",
        "'''\n",
        "#LOG_DIR = SOURCE + 'logs'\n",
        "#metadata = os.path.join(LOG_DIR, 'metadata.tsv')\n",
        "#config = projector.ProjectorConfig()\n",
        "\n",
        "log_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
        "w2vp = W2V_Pipeline(load_model = False, save_model = True)\n",
        "embed_weights = w2vp(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWSpd9MecA8b"
      },
      "source": [
        "## W2V Dash "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRGlsN0McF1D"
      },
      "source": [
        "### W2V Dash Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9AAeUOmnz4h"
      },
      "source": [
        "def tree_parser(node, l, m, root_node):\n",
        "    d = node.key_to_child_node # dict\n",
        "    for token in list(d.keys()):\n",
        "        if len(root_node.key_to_child_node.keys()) == 0:\n",
        "            ret_list = []\n",
        "            for row in m:\n",
        "                proper_len = int(row[1])\n",
        "                if len(row) == proper_len+1:\n",
        "                  ret_list.append(row)\n",
        "            return ret_list\n",
        "        l.append(token)\n",
        "        child = d[token]\n",
        "        if child.key_to_child_node:\n",
        "            tree_parser(child, l, m, root_node)\n",
        "        else:\n",
        "            d.pop(token)\n",
        "            m.append(l)\n",
        "            l = ['root']\n",
        "            tree_parser(root_node, l, m, root_node)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRaXmjdkcvVm"
      },
      "source": [
        "### W2V Dash Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCgMbFevmC88"
      },
      "source": [
        "The output of the W2V pipeline is a matrix of size [vocab size x embedding size] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw4mwJz6eGa6"
      },
      "source": [
        "# -- W2V Dash Environmental Variables -- #\n",
        "\n",
        "W2V_NEIGHBORS = 20\n",
        "RECURSION_LIMIT = 10**6\n",
        "N_PROJ_DIM = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a60tfcLUoERG"
      },
      "source": [
        "# -- Generate Data for Word Embeddings Projector -- #\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# shape = vocab size x embedding dim size\n",
        "weights = np.ndarray(shape=(len(embed_weights), W2V_EMBED_SIZE)) \n",
        "pca = PCA(n_components=N_PROJ_DIM)\n",
        "\n",
        "# -- Populate Matrix for PCA -- #\n",
        "for idx, weight in enumerate(list(embed_weights.values())):\n",
        "    weights[idx, :] = weight \n",
        "\n",
        "reduced_embeddings = pca.fit_transform(weights)\n",
        "\n",
        "# -- Calculate Nearest Neighbors -- #\n",
        "embeddings_knn = NearestNeighbors(n_neighbors=W2V_NEIGHBORS, algorithm='auto')\n",
        "embeddings_knn_trained = embeddings_knn.fit(reduced_embeddings)\n",
        "\n",
        "# Currently the array has a shape of vocab size x N_PROJ_DIM and contains\n",
        "# the fitted PCA data. We need to add the vocab in the first column so \n",
        "# we know which vectors are represented. \n",
        "embedding_vocab_arr = np.array(list((embed_weights.keys())))\n",
        "embedding_vocab_arr = np.expand_dims(embedding_vocab_arr, 1)\n",
        "reduced_embeddings = np.hstack((embedding_vocab_arr, reduced_embeddings))\n",
        "\n",
        "# This dataframe will be passed to the projection plot\n",
        "plot_df = pd.DataFrame(data=reduced_embeddings, \n",
        "                       columns=['token', 'x1', 'x2', 'x3'])\n",
        "\n",
        "# We want to ensure that the coordinates are numerical \n",
        "plot_df['x1'] = pd.to_numeric(plot_df['x1'])\n",
        "plot_df['x2'] = pd.to_numeric(plot_df['x2'])\n",
        "plot_df['x3'] = pd.to_numeric(plot_df['x3'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC17hivjjKs0"
      },
      "source": [
        "We will build our plot using the tree_parser function. This function recursively\n",
        "steps through the drain3.TemplateMiner.drain.Node structure of our \n",
        "**TextClusteringLayer** (TCL). The recursion populates a np.array which is then used\n",
        "to build a pandas dataframe which the plotly treemap accepts. There is a column\n",
        "appended to the tail of the dataframe which counts the number of stars \n",
        "(wild card masks) present in the row. This is used to define the colors shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7TkKEOFyRH3"
      },
      "source": [
        "# -- Generate Data for Treemap -- #\n",
        "\n",
        "# By default python's recursion limit is 10**4 which is too small for our needs\n",
        "sys.setrecursionlimit(RECURSION_LIMIT)\n",
        "\n",
        "'''\n",
        "We start by defining the head of our tree which is root. The list m is our \n",
        "master list for recording the paths in the tree. Each path is another list \n",
        "stored here.\n",
        "'''\n",
        "\n",
        "l = ['root']\n",
        "m = []\n",
        "\n",
        "# The root node is the master node of the tree and will be our return point\n",
        "root_node = deepcopy(w2vp.TCL.template_miner.drain.root_node)\n",
        "\n",
        "parsed_tree = tree_parser(root_node, l, m, root_node)\n",
        "parsed_tree_df = pd.DataFrame(data=parsed_tree)\n",
        "\n",
        "# The returned dataframe has generic columns so we will provide custom labels\n",
        "n_cols = len(parsed_tree_df.columns)\n",
        "col_name_list = []\n",
        "for idx in range(n_cols):\n",
        "    col_name_list.append('level' + str(idx))\n",
        "parsed_tree_df.columns = col_name_list\n",
        "\n",
        "'''\n",
        "Without a color column our treemap would just be plain. We thought that taking \n",
        "the sum of the drain mask would be an interesting way to color the treemap. This\n",
        "lambda function will sum those values in each row and return them to a new \n",
        "columnn named 'sum'\n",
        "'''\n",
        "parsed_tree_df['sum'] = parsed_tree_df.apply(lambda x: x.str.contains('<*>'), axis=1).sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je-dqJAGlzA9"
      },
      "source": [
        "import dash\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from dash.dependencies import Input, Output, State\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "import json\n",
        "import dash_table\n",
        "\n",
        "external_stylesheets = ['https://drive.google.com/uc?export=view&id=19OXGQ5iJIjRZD4VEZ-xiVChDmj0-SlSF']\n",
        "\n",
        "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
        "\n",
        "f = px.scatter_3d(\n",
        "    plot_df,\n",
        "    x='x1', \n",
        "    y='x2', \n",
        "    z='x3',\n",
        "    hover_name='token',\n",
        "    height=750)\n",
        "\n",
        "f.update_traces(marker=dict(size=5, line=dict(width=2, color='DarkSlateGrey')))\n",
        "\n",
        "# -- TreeMap -- #\n",
        "# f = px.treemap(parsed_tree_df, path=col_name_list, color='sum')\n",
        "\n",
        "table = pd.DataFrame(data=list(embed_weights.keys()), columns=['token'])\n",
        "\n",
        "app.layout = html.Div([\n",
        "        html.Div(dcc.Graph(id = '3d_scat', figure=f, style={'height': 'auto'}), className='color1', id='graph_div'),\n",
        "        html.Div(className='color2', id = 'data_table',\n",
        "                children=[dash_table.DataTable(\n",
        "                    id='table',\n",
        "                    columns=[{\"name\": i, \"id\": i} for i in table.columns],\n",
        "                    data=pd.DataFrame().to_dict('records'),\n",
        "                    style_cell={'textAlign': 'left', 'overflow': 'hidden',\n",
        "                                'textOverflow': 'ellipsis',\n",
        "                                'maxWidth': 0})])], className='container')\n",
        "\n",
        "# style_data={'height': 'auto', 'lineHeight': '15px'}\n",
        "                    # style_table={'height': 'auto'},\n",
        "                    # table.to_dict('records')\n",
        "global_update = None\n",
        "# html.Div(className='color3', id='logger', children=[html.P(children=\"this is a temporary log\", id='logger_text')])\n",
        "\n",
        "@app.callback(Output(\"table\", \"data\"),\n",
        "              Output(\"3d_scat\", \"figure\"),\n",
        "              Output(\"logger_text\", \"children\"),\n",
        "            Input('3d_scat', 'clickData'))\n",
        "def select_point(clickData):\n",
        "    ctx = dash.callback_context\n",
        "    ids = [c['prop_id'] for c in ctx.triggered]\n",
        "\n",
        "    if '3d_scat.clickData' in ids:\n",
        "        if clickData:\n",
        "            for p in clickData['points']:\n",
        "                l = [p['x'],p['y'],p['z']]\n",
        "                query_arr = np.array(l).reshape(1,-1)\n",
        "                _, neighbors = knn_trained.kneighbors(X=query_arr)\n",
        "                neighbors_list = neighbors.tolist()[0]\n",
        "                tokens = []\n",
        "                for idx in neighbors_list:\n",
        "                    tokens.append(table.iloc[idx])\n",
        "                update = pd.DataFrame(data=tokens)\n",
        "                new_df = plot_df[plot_df.index.isin(neighbors_list)]\n",
        "                old_df = plot_df.drop(index=neighbors_list)\n",
        "                \n",
        "                ff = px.scatter_3d(\n",
        "                    new_df,\n",
        "                    x='x1', \n",
        "                    y='x2', \n",
        "                    z='x3',\n",
        "                    hover_name='token',\n",
        "                    height=750)\n",
        "                \n",
        "                ff = ff.update_traces(marker=dict(size=5, color='red', line=dict(width=2, color='blue')))\n",
        "\n",
        "                ff2 = px.scatter_3d(\n",
        "                    old_df,\n",
        "                    x='x1', \n",
        "                    y='x2', \n",
        "                    z='x3',\n",
        "                    hover_name='token',\n",
        "                    height=750)\n",
        "                \n",
        "                ff2 = ff2.update_traces(marker=dict(size=5, color='purple', opacity=0.2, line=dict(width=2, color='orange')))\n",
        "\n",
        "                ff.add_trace(ff2.data[0])\n",
        "\n",
        "                return update.to_dict('records'), ff, str(old_df.head(5))\n",
        "\n",
        "app.run_server(mode='inline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZNPHgwy3XXZ"
      },
      "source": [
        "# Transformer Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmiGMB4_-l8x"
      },
      "source": [
        "### Main (Initialization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7hXYiqW3o6M"
      },
      "source": [
        "# -- Data Batches, Vocab, and Embedding -- #\n",
        "word_embedding_matrix = joblib.load(SOURCE + \"results/w2v_weights.joblib\")\n",
        "vocabulary = joblib.load(SOURCE + \"results/vocab_dict.joblib\")\n",
        "dataset = database_builder(SOURCE + 'database/')\n",
        "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "batched_dataset = process_all_batches()\n",
        "\n",
        "# -- Transformer Model -- #\n",
        "optimus_prime = Transformer(\n",
        "    TRANSFORMER_LAYERS,\n",
        "    W2V_EMBED_SIZE,\n",
        "    TRANSFORMER_HEADS,\n",
        "    TRANSFORMER_DFF,\n",
        "    vocab_size,\n",
        "    word_embedding_matrix,\n",
        "    MAX_SEQ_LEN,\n",
        "    DROPOUT_RATE)\n",
        "\n",
        "# -- Labels -- #\n",
        "label_unique = dataset['label'].unique()\n",
        "lbp = LabelEncoder().fit(label_unique)\n",
        "binary_labels = lbp.transform(label_unique)\n",
        "\n",
        "log_labels = {}\n",
        "for idx, label in enumerate(label_unique):\n",
        "    log_labels.update({\n",
        "        label: binary_labels[idx]\n",
        "    })\n",
        "\n",
        "# -- Model Metrics -- #\n",
        "learning_rate = CustomSchedule(W2V_EMBED_SIZE)\n",
        "epoch_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "epoch_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# -- Classification Step Layers -- #\n",
        "add_att_layer = tf.keras.layers.AdditiveAttention()\n",
        "softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "s1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(BATCH_SIZE, activation=ACTIVATION),\n",
        "    tf.keras.layers.Dense(4, activation=ACTIVATION),\n",
        "    tf.keras.layers.Softmax()\n",
        "])\n",
        "\n",
        "# -- Pipeline Info -- #\n",
        "n_logs = len(dataset.index)\n",
        "#n_iter = n_logs // BATCH_SIZE\n",
        "n_iter = 5\n",
        "remainder = n_logs % BATCH_SIZE\n",
        "attns = []\n",
        "\n",
        "\n",
        "# -- Checkpoints -- #\n",
        "checkpoint_path = SOURCE + \"checkpoints/\"\n",
        "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), transformer=optimus_prime, optimizer=optimizer)\n",
        "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "tf.debugging.set_log_device_placement(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtCEbgsytePO"
      },
      "source": [
        "def process_all_batches():\n",
        "    batches = []\n",
        "\n",
        "    for idx in range(n_iter + 1):\n",
        "        log_batch, labels = process_batch(dataset, vocabulary, idx, log_labels)\n",
        "\n",
        "        batches.append((log_batch, labels))\n",
        "\n",
        "    return batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0dMK9zKv13C"
      },
      "source": [
        "    tf.profiler.experimental.stop()\n",
        "    tf.summary.trace_off()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKEF1GYqtXSv"
      },
      "source": [
        "### Main (Training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dc-ZONUfLHe"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    start = time.time()\n",
        "    epoch_loss.reset_states()\n",
        "    epoch_accuracy.reset_states()\n",
        "    dataset_iter = iter(batched_dataset)\n",
        "\n",
        "    t = tqdm(range(n_iter), desc=\"Epoch: {:03d}, Loss: {:.3f}, Accuracy: {:.3%}\".format(0, 0, 0), position=0, leave=True)\n",
        "    for _ in t:\n",
        "        batch = next(dataset_iter)\n",
        "        log_batch = batch[0]\n",
        "        labels = batch[1]\n",
        "\n",
        "        # Returns Eager Tensor for Predictions\n",
        "        tf.summary.trace_on()\n",
        "        tf.profiler.experimental.start(logdir)\n",
        "        with writer.as_default():\n",
        "          train_step(log_batch, labels)\n",
        "          # with tf.summary.record_if(True):\n",
        "\n",
        "          tf.summary.trace_export(\n",
        "            name = \"training_trace\",\n",
        "            step=0,\n",
        "            profiler_outdir=logdir\n",
        "          )\n",
        "\n",
        "        tf.profiler.experimental.stop()\n",
        "        tf.summary.trace_off()\n",
        "        \n",
        "        checkpoint.step.assign_add(1)\n",
        "\n",
        "        if int(checkpoint.step) % 10 == 0:\n",
        "            save_path = checkpoint_manager.save()\n",
        "\n",
        "        t.set_description(desc=\"Epoch: {:03d}, Loss: {:.3f}, Accuracy: {:.3%} \".format(epoch,\n",
        "                                                                    epoch_loss.result(),\n",
        "                                                                    epoch_accuracy.result()))\n",
        "        t.refresh()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT-fshbn3WUy"
      },
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=([BATCH_SIZE, None]), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=([BATCH_SIZE]), dtype=tf.float32)\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)#, experimental_compile=True)\n",
        "def train_step(log_batch: tf.Tensor, \n",
        "               labels: tf.Tensor):\n",
        "    transformer_input = tf.tuple([\n",
        "        log_batch,  # <tf.Tensor: shape=(batch_size, max_seq_len), dtype=float32>\n",
        "        labels  # <tf.Tensor: shape=(batch_size, num_classes), dtype=float32>\n",
        "    ])\n",
        "    with tf.GradientTape() as tape:\n",
        "        Rs, _ = optimus_prime(transformer_input)\n",
        "        a_s = add_att_layer([Rs, Rs])\n",
        "        y = softmax(a_s * Rs)\n",
        "        print(a_s.shape)\n",
        "        # y = Rs\n",
        "        loss = tf.py_function(loss_function, [labels, y], tf.float32)\n",
        "        pred = s1(y)\n",
        "        labels = tf.cast(labels, tf.int64)\n",
        "    # Optimize the model\n",
        "    grads = tape.gradient(loss, optimus_prime.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, optimus_prime.trainable_variables))\n",
        "\n",
        "    acc = accuracy_function(labels, pred)\n",
        "\n",
        "    # Tracking Progress\n",
        "    epoch_loss.update_state(loss)  # Adding Batch Loss\n",
        "    epoch_accuracy.update_state(acc)\n",
        "\n",
        "    # return loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K5ODbmo_hWj"
      },
      "source": [
        "## Metric Objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5QbcqbJ_vZH"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WytrcTtl-4q2"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Jm1yHR_wou"
      },
      "source": [
        "### Accuracy Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZodkFf-_09c"
      },
      "source": [
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MubVySBE_wVH"
      },
      "source": [
        "### Custom Learning Rate Schedule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FjRZs2A_286"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model: int, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JtgL-iQkqj2"
      },
      "source": [
        "## Pipeline Objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZGIBu-d_AEK"
      },
      "source": [
        "### ProcessBatch (NEEDS UPDATE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk9w4xg_3cMD"
      },
      "source": [
        "def process_batch(dataset: pd.DataFrame,\n",
        "                  vocabulary: dict,\n",
        "                  idx: int,\n",
        "                  labels: dict) -> tuple:\n",
        "    logs = np.zeros((BATCH_SIZE, MAX_SEQ_LEN))\n",
        "    y_true = np.empty((BATCH_SIZE,))\n",
        "\n",
        "    start_window = idx * BATCH_SIZE\n",
        "    end_window = (idx + 1) * BATCH_SIZE\n",
        "    for log_idx, log in enumerate(dataset['log'][start_window:end_window]):\n",
        "        for seq_idx, word in enumerate(log.split()):\n",
        "            if seq_idx >= MAX_SEQ_LEN:\n",
        "                break\n",
        "            logs[log_idx, seq_idx] = vocabulary[word] if word in vocabulary.keys() else 0\n",
        "        y_true[log_idx] = labels[dataset['label'][log_idx]]\n",
        "\n",
        "    return tf.convert_to_tensor(logs, dtype=tf.float32), tf.convert_to_tensor(y_true, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-Dg1Sc6_QU2"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRk21bVC2oWu"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 dff,\n",
        "                 input_vocab_size,\n",
        "                 embedding_matrix,\n",
        "                 max_seq_len,\n",
        "                 rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # self.embedding = tf.keras.layers.Embedding(\n",
        "        #     input_vocab_size,\n",
        "        #     d_model,\n",
        "        #     weights=[embedding_matrix],\n",
        "        #     input_length=max_seq_len,\n",
        "        #     trainable=False)\n",
        "        \n",
        "        self.embedding = EmbeddingLayer(input_vocab_size, d_model, embedding_matrix, max_seq_len)\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "                        num_layers,\n",
        "                        d_model,\n",
        "                        embedding_matrix,\n",
        "                        num_heads,\n",
        "                        dff,\n",
        "                        input_vocab_size,\n",
        "                        max_seq_len,\n",
        "                        rate) for _ in range(3)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, input_tuple: tf.tuple, **kwargs):\n",
        "        log_batch = input_tuple[0]\n",
        "        encoding_padding_mask = None # input_tuple[1]\n",
        "        \n",
        "        embedding_tensor = self.embedding(log_batch) # (batch_size, input_seq_len, d_model)\n",
        "        embedding_tensor = self.pos_encoding(embedding_tensor)\n",
        "        embedding_tensor = self.dropout(embedding_tensor, training=TRAINING)\n",
        "\n",
        "        # Transformer Block #1\n",
        "        # (batch_size, inp_seq_len, d_model), (batch_size, class, inp_seq_len, inp_seq_len)\n",
        "        enc_output, att = self.transformer_blocks[0](embedding_tensor, encoding_padding_mask)\n",
        "\n",
        "        # Transformer Block #2 vv (takes the place of the Decoder)\n",
        "        fin_output, att = self.transformer_blocks[1](enc_output, encoding_padding_mask)\n",
        "\n",
        "        final_output = tf.reduce_mean(fin_output, axis=1)\n",
        "        final_output = tf.expand_dims(final_output, axis=0)\n",
        "\n",
        "        print(final_output.shape)\n",
        "\n",
        "        out, att = self.transformer_blocks[2](final_output, encoding_padding_mask)\n",
        "\n",
        "        seq_representation = tf.reduce_mean(out, axis=1)\n",
        "        return seq_representation, att"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vf9VgUB-6OD"
      },
      "source": [
        "### EmbeddingLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsMsrJNHstXc"
      },
      "source": [
        "class EmbeddingLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, d_model, embedding_matrix, max_seq_len):\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(\n",
        "      input_vocab_size,\n",
        "      d_model,\n",
        "      weights=[embedding_matrix],\n",
        "      input_length=max_seq_len,\n",
        "      trainable=False)\n",
        "\n",
        "  def call(self, input):\n",
        "    input_sequences = log_tokenizer.texts_to_sequences(input)\n",
        "    \n",
        "    inputs = pad_sequences(input_sequences, maxlen=self.max_seq_len, padding='post')\n",
        "\n",
        "    embedding_tensor = self.embedding(inputs)\n",
        "    embedding_tensor *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return embedding_tensor\n",
        "\n",
        "  # adding embedding and position encoding.\n",
        "  # embedding_tensor = self.embedding(log_batch, training=TRAINING)  # (batch_size, input_seq_len, d_model)\n",
        "  # embedding_tensor *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zzi2tYRAYC1"
      },
      "source": [
        "### PositionalEncodingLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyQUMo4Aarf"
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if max_dims % 2 == 1: max_dims += 1  # max_dims must be even\n",
        "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "        pos_emb = np.empty((1, max_steps, max_dims))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)\n",
        "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksz4Lk2K_WV6"
      },
      "source": [
        "### TransformerBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77qno34G23WG"
      },
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_layers,\n",
        "                 d_model,\n",
        "                 embedding_matrix,\n",
        "                 num_heads,\n",
        "                 dff,\n",
        "                 input_vocab_size,\n",
        "                 max_seq_len,\n",
        "                 rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        attn_weights = None\n",
        "        for i in range(self.num_layers):\n",
        "            x, attn_weights = self.enc_layers[i](x, mask)\n",
        "\n",
        "        return tf.convert_to_tensor(x), tf.convert_to_tensor(attn_weights)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARsH5IxX_Th0"
      },
      "source": [
        "### EncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSmeFr022zA0"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 d_model: int,\n",
        "                 num_heads: int,\n",
        "                 dff: int,\n",
        "                 rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.multi_headed_attention = MultiHeadAttention(num_heads=num_heads,\n",
        "                                                         key_dim=d_model // num_heads,\n",
        "                                                         dropout=0.1)\n",
        "\n",
        "        self.feed_forward_network = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation=ACTIVATION),  # (batch_size, seq_len, dff)\n",
        "            tf.keras.layers.Dense(d_model, activation=ACTIVATION)  # (batch_size, seq_len, d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        # (1) - Attention Score\n",
        "        attn_output, attn_weights = self.multi_headed_attention(x, \n",
        "                                                                x, \n",
        "                                                                return_attention_scores=True)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # (2) - Add & Normalize\n",
        "        attn_output = self.dropout1(attn_output, training=TRAINING)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # (3) - Feed Forward NN\n",
        "        feed_forward_output = self.feed_forward_network(out1)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # (4) - Add & Normalize\n",
        "        feed_forward_output = self.dropout2(feed_forward_output, training=TRAINING)\n",
        "        out2 = self.layernorm2(out1 + feed_forward_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return tf.convert_to_tensor(out2), tf.convert_to_tensor(attn_weights)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}