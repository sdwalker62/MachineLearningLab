{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejXAKpdM7vpW"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZmbFWc97xsl"
   },
   "source": [
    "Feel free to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"logs\n0    This is a test\n1     Test number 1\n2       Please work\n3  lol what is this\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-137a69897a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marr_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/env/lib/python3.8/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromMap2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.8/site-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer__TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Not found: \"logs\n0    This is a test\n1     Test number 1\n2       Please work\n3  lol what is this\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "arr_list = [\"This is a test\", \"Test number 1\", \"Please work\", \"lol what is this\"]\n",
    "arr_df = pd.DataFrame(arr_list, columns=[\"logs\"])\n",
    "\n",
    "spm.SentencePieceTrainer.train(input=arr_df, model_prefix='m', vocab_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QLnvgfRy2U1H"
   },
   "outputs": [],
   "source": [
    "# -- Base -- #\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# -- Metrics -- #\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "import tensorboard\n",
    "\n",
    "# -- Tensorflow -- #\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow \n",
    "\n",
    "# -- Misc Models -- #\n",
    "import drain3\n",
    "from gensim.models.phrases import Phrases\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# -- Dash -- #\n",
    "import dash\n",
    "import dash_table\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from dash import no_update\n",
    "from flask_caching import Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L19JQx8YwmH"
   },
   "source": [
    "Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0N_5Z7Gg4Jl5"
   },
   "source": [
    "## Environmental Variables\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xvZC0zAR4NUe"
   },
   "outputs": [],
   "source": [
    "SOURCE = '/home/' + os.environ['USER'] + '/app'\n",
    "\n",
    "# -- TRANSFORMER Pipeline -- #\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "DROPOUT_RATE = 0.1\n",
    "MAX_SEQ_LEN = 200\n",
    "\n",
    "ACTIVATION = \"elu\"\n",
    "\n",
    "TRANSFORMER_LAYERS = 4\n",
    "TRANSFORMER_DFF = 2000\n",
    "TRANSFORMER_HEADS = 8\n",
    "\n",
    "TRAINING = True\n",
    "CONTAINER = 'core.soaesb'\n",
    "\n",
    "# -- WORD2VEC Pipeline -- #\n",
    "WINDOW_SIZE = 10\n",
    "GENERATE_NEW_DRAIN = True\n",
    "NUM_NEGATIVE_SAMPLING = 10\n",
    "W2V_BATCH_SIZE = 2048\n",
    "BUFFER_SIZE = 10000\n",
    "W2V_EPOCHS = 200\n",
    "W2V_EMBED_SIZE = 512 #64\n",
    "MAX_VOCAB_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "HM0ON9lGYvGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "# Set up logging.\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = SOURCE + 'logs/func/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wOAfM3VZIWl"
   },
   "source": [
    "view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "4uJdL8IGZKdL"
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir /content/drive/MyDrive/Work/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdlwPqq97n1m"
   },
   "source": [
    "Check if GPU is in use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "p1kUaw097qmg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngpu_info = !nvidia-smi\\ngpu_info = \\'\\n\\'.join(gpu_info)\\nif gpu_info.find(\\'failed\\') >= 0:\\n  print(\\'Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, \\')\\n  print(\\'and then re-execute this cell.\\')\\nelse:\\n  print(gpu_info)\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8EnuKFhAuER",
    "tags": []
   },
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "GUivYRdyAstr"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s | %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITSJA5hWASDn",
    "tags": []
   },
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnCJ678Ka-T1"
   },
   "source": [
    "## Define Database Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Gwi7HlrBkwwl"
   },
   "outputs": [],
   "source": [
    "def database_builder(path: str) -> pd.DataFrame():\n",
    "    logger.info('Building DataFrame ...')\n",
    "    (_, _, files) = next(os.walk(path))\n",
    "    sql_query = 'SELECT * FROM logs'\n",
    "    data = []\n",
    "    for f in files:\n",
    "        if '.db' in f:\n",
    "            conn = create_connection(path + f)\n",
    "            d = pd.read_sql_query(sql_query, conn)\n",
    "            data.append(d)\n",
    "    logger.info('...complete!')\n",
    "    return pd.concat(data)\n",
    "\n",
    "\n",
    "def create_connection(path: str) -> sql.Connection:\n",
    "    \"\"\"\n",
    "    Creates a database connection\n",
    "    :param path: str\n",
    "        path to database object\n",
    "    :return sql.Connection\n",
    "        a connection to the database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sql.connect(path)\n",
    "        logger.info('Connected to database ' + path)\n",
    "        return conn\n",
    "    except sql.Error as e:\n",
    "        logger.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekFXItHibG7r"
   },
   "source": [
    "## Define Dataset Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50ge2uap_kTN",
    "outputId": "af603cfd-5c79-4772-b0d7-9ab271eb4f4f"
   },
   "outputs": [],
   "source": [
    "dataset = database_builder(SOURCE + '/data/')\n",
    "container_dataset = dataset[dataset['container_name'] == CONTAINER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkC2MdmCBR7C"
   },
   "source": [
    "# W2V Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wFAAOw-q3w5",
    "tags": []
   },
   "source": [
    "## Pipeline Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxst8UQeq8sL"
   },
   "source": [
    "### Standardize Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FIbxj60aLAfC"
   },
   "outputs": [],
   "source": [
    "def standardize_logs(logs: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # remove timestamps\n",
    "    logs['log'] = logs['log'].replace(\n",
    "        to_replace=r'(?:\\d{4}-\\d{2}-\\d{2}[\\sT]\\d{2}:\\d{2}:\\d{2}([.,]\\d{3}|\\s))|(?:\\s{2,})',\n",
    "        value=' ',\n",
    "        regex=True)\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLKVLZioq_2i"
   },
   "source": [
    "### PhraseCaptureLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gFTBqUREEHLn"
   },
   "outputs": [],
   "source": [
    "class PhraseCaptureLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 min_count,\n",
    "                 threshold,\n",
    "                 load_model=False,\n",
    "                 save_model=True):\n",
    "\n",
    "        super(PhraseCaptureLayer, self).__init__()\n",
    "        self.min_count = min_count\n",
    "        self.threshold = threshold\n",
    "        self.load_model = load_model\n",
    "        self.save_model = save_model\n",
    "\n",
    "        if self.load_model:\n",
    "            self.phrase_model = joblib.load(SOURCE + '/results/phrase_model.joblib')\n",
    "        else:\n",
    "            self.phrase_model = Phrases(min_count=self.min_count, threshold=self.threshold)\n",
    "\n",
    "    def call(self, corpus, training=True):\n",
    "\n",
    "        def clean_log(log):\n",
    "            log = log.lower().strip()\n",
    "            return re.sub(r'\\s{2,}', ' ', log)\n",
    "\n",
    "        def reorganize_return(corpus_with_phrases):\n",
    "            l = []\n",
    "            for tokenized_log in corpus_with_phrases:\n",
    "                l.append(' '.join(tokenized_log))\n",
    "            return l\n",
    "\n",
    "        split_corpus = [log.split(' ') for log in corpus['log']]\n",
    "\n",
    "        if not training:\n",
    "            self.phrase_model = self.phrase_model.freeze()\n",
    "\n",
    "        self.phrase_model.add_vocab(split_corpus)\n",
    "\n",
    "        if self.save_model:\n",
    "            joblib.dump(self.phrase_model, SOURCE + '/results/phrase_model.joblib')\n",
    "\n",
    "        corpus_with_phrases = self.phrase_model.__getitem__(split_corpus)\n",
    "        return reorganize_return(corpus_with_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb6Ncm7-rKLZ"
   },
   "source": [
    "### TextClusteringLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nGOICjK9MtxY"
   },
   "outputs": [],
   "source": [
    "class TextClusteringLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 load_model=False,\n",
    "                 save_model=True):\n",
    "\n",
    "        super(TextClusteringLayer, self).__init__()\n",
    "        self.load_model = load_model\n",
    "        self.save_model = save_model\n",
    "\n",
    "        if load_model:\n",
    "            self.template_miner = joblib.load(SOURCE +\n",
    "                                              '/results/template_miner.joblib')\n",
    "        else:\n",
    "            self.template_miner = drain3.TemplateMiner()\n",
    "\n",
    "    def call(self, corpus, training=True):\n",
    "        if training:\n",
    "            for log in corpus:\n",
    "                self.template_miner.add_log_message(log)\n",
    "            if self.save_model:\n",
    "                joblib.dump(self.template_miner, SOURCE +\n",
    "                            '/results/template_miner.joblib')\n",
    "\n",
    "            for idx, log in enumerate(corpus):\n",
    "                template = self.template_miner.match(log).get_template()\n",
    "                corpus[idx] = template\n",
    "\n",
    "            return [re.sub(pattern=r' +',\n",
    "                           repl=' ',\n",
    "                           string=cluster) for cluster in corpus]\n",
    "        else:\n",
    "            log_list = []\n",
    "            for log in corpus:\n",
    "                match_cluster = self.template_miner.match(log)\n",
    "                if match_cluster is None:\n",
    "                    match_cluster = self.template_miner.add_log_message(log)\n",
    "                log_list.append(match_cluster)\n",
    "            return [re.sub(pattern=r' +',\n",
    "                           repl=' ',\n",
    "                           string=cluster.get_template()) for cluster in log_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl_BXs9DrSA5"
   },
   "source": [
    "### NegativeSkipgramLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GAfbeuTcnFQK"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NSLBundle:\n",
    "    vocab: dict\n",
    "    targets: list\n",
    "    contexts: list\n",
    "    labels: list\n",
    "\n",
    "\n",
    "class NegativeSkipgramLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 window_size=10,\n",
    "                 save_data=True):\n",
    "\n",
    "        super(NegativeSkipgramLayer, self).__init__()\n",
    "        self.vocab_size = 0\n",
    "        self.vectorized_logs, self.corpus = [], []\n",
    "        self.targets, self.contexts, self.labels = [], [], []\n",
    "        self.vocab = {}\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.save_data = save_data\n",
    "\n",
    "    def collect_vocabulary(self):\n",
    "        self.vocab[0] = '<pad>'\n",
    "\n",
    "        log_tokenizer.fit_on_texts(self.corpus)\n",
    "        self.vectorized_logs = log_tokenizer.texts_to_sequences(self.corpus)\n",
    "\n",
    "        self.vocab.update({v: k for k, v in log_tokenizer.word_index.items()})\n",
    "        self.vocab_size = len(self.vocab.keys())\n",
    "\n",
    "    def find_word_context(self):\n",
    "\n",
    "        # Build the sampling table for vocab_size tokens.\n",
    "        sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(len(self.vocab))\n",
    "\n",
    "        for sequence in tqdm(self.vectorized_logs, position=0, leave=True):\n",
    "\n",
    "            positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "                sequence,\n",
    "                vocabulary_size=len(self.vocab),\n",
    "                sampling_table=sampling_table,\n",
    "                window_size=self.window_size,\n",
    "                negative_samples=0)\n",
    "\n",
    "            for target_word, context_word in positive_skip_grams:\n",
    "                context_class = tf.expand_dims(\n",
    "                    tf.constant([context_word], dtype='int64'), 1)\n",
    "\n",
    "                negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                    true_classes=context_class,\n",
    "                    num_true=1,\n",
    "                    num_sampled=NUM_NEGATIVE_SAMPLING,\n",
    "                    unique=True,\n",
    "                    range_max=len(self.vocab),\n",
    "                    seed=42,\n",
    "                    name=\"negative_sampling\")\n",
    "\n",
    "                negative_sampling_candidates = tf.expand_dims(\n",
    "                    negative_sampling_candidates, 1)\n",
    "\n",
    "                context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "                label = tf.constant([1] + [0] * NUM_NEGATIVE_SAMPLING, dtype='int64')\n",
    "\n",
    "                self.targets.append(target_word)\n",
    "                self.contexts.append(context)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def call(self, corpus, training = True):\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.collect_vocabulary()\n",
    "        self.find_word_context()\n",
    "\n",
    "        if self.save_data:\n",
    "            joblib.dump(self.vocab, SOURCE + '/results/vocab.joblib')\n",
    "            joblib.dump(self.targets, SOURCE + '/results/targets.joblib')\n",
    "            joblib.dump(self.contexts, SOURCE + '/results/contexts.joblib')\n",
    "            joblib.dump(self.labels, SOURCE + '/results/labels.joblib')\n",
    "\n",
    "        return NSLBundle(self.vocab, self.targets, self.contexts, self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxXEjBM_rWYC"
   },
   "source": [
    "### Word2VecEmbeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-JyKVfDf8CaP"
   },
   "outputs": [],
   "source": [
    "class Word2VecEmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, \n",
    "                 embedding_dim,\n",
    "                 load_model = False, \n",
    "                 save_model = True):\n",
    "\n",
    "        super(Word2VecEmbeddingLayer, self).__init__()\n",
    "        self.embeddings = {}\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.load_model = load_model\n",
    "        self.save_model = save_model\n",
    "        self.Optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        if load_model:\n",
    "            self.Word2Vec = load_model(SOURCE + '/results/word2vec')\n",
    "        else:\n",
    "            self.Word2Vec= None\n",
    "\n",
    "    def call(self, in_bundle, training):\n",
    "\n",
    "        vocab = in_bundle.vocab\n",
    "        targets = in_bundle.targets\n",
    "        contexts = in_bundle.contexts\n",
    "        labels = in_bundle.labels\n",
    "\n",
    "        if not self.load_model and self.Word2Vec is None:\n",
    "            self.Word2Vec = Word2Vec(len(vocab.keys()), self.embedding_dim)\n",
    "            self.Word2Vec.compile(\n",
    "                optimizer=self.Optimizer,\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE).batch(W2V_BATCH_SIZE, drop_remainder=True)\n",
    "        dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        self.Word2Vec.fit(dataset, epochs=W2V_EPOCHS)\n",
    "        weights = self.Word2Vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "\n",
    "        for word in vocab.items():\n",
    "            self.embeddings.update({\n",
    "                word[1]: weights[word[0]]\n",
    "                })\n",
    "\n",
    "        if self.save_model:\n",
    "            self.Word2Vec.save(SOURCE + '/results/word2vec')\n",
    "            out_v = io.open(SOURCE + '/results/vectors.tsv', 'w', encoding='utf-8')\n",
    "            out_m = io.open(SOURCE + '/results/metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "            for index, word in enumerate(vocab.values()):\n",
    "                if index == 0:\n",
    "                    continue  # skip 0, it's padding.\n",
    "                vec = weights[index]\n",
    "                out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "                out_m.write(word + \"\\n\")\n",
    "            out_v.close()\n",
    "            out_m.close()\n",
    "\n",
    "        self.Word2Vec.summary()\n",
    "        return self.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hQRAbPfrZwt"
   },
   "source": [
    "### Word2VecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "H_GiqTup8AY3"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            input_length=1, # input length 1 since we are focusing on one token\n",
    "            name=\"w2v_embedding\")\n",
    "\n",
    "        self.context_embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            input_length=NUM_NEGATIVE_SAMPLING + 1) # window size for contextual \n",
    "            # reasoning behind the sample token\n",
    "        self.dots = tf.keras.layers.Dot(axes=(3, 2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        we = self.target_embedding(target)\n",
    "        ce = self.context_embedding(context)\n",
    "        dots = self.dots([ce, we])\n",
    "        return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvhQ3bvitv5b"
   },
   "source": [
    "### W2V_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "R_nmehscsdmv"
   },
   "outputs": [],
   "source": [
    "class W2V_Pipeline(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 save_model,\n",
    "                 load_model):\n",
    "        \n",
    "        super(W2V_Pipeline, self).__init__()\n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        \n",
    "        self.PCL = PhraseCaptureLayer(\n",
    "            5, 7, \n",
    "            load_model=load_model, \n",
    "            save_model=save_model)\n",
    "        \n",
    "        self.TCL = TextClusteringLayer(\n",
    "            load_model=load_model, \n",
    "            save_model=save_model)\n",
    "        \n",
    "        self.NSL = NegativeSkipgramLayer(W2V_EMBED_SIZE)\n",
    "\n",
    "        self.W2V = Word2VecEmbeddingLayer(\n",
    "            W2V_EMBED_SIZE, \n",
    "            load_model=load_model, \n",
    "            save_model=save_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = standardize_logs(x)\n",
    "        x = self.PCL(x)\n",
    "        x = self.TCL(x)\n",
    "        x = self.NSL(x)\n",
    "        return self.W2V(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>container_name</th>\n",
       "      <th>log</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-01-29T18:07:13.134Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:08,402 | INFO  | aging/0-SNAP...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2021-01-29T18:07:13.134Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:08,402 | INFO  | aging/0-SNAP...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2021-01-29T18:07:13.134Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:05,257 | INFO  | ev.HealthMon...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2021-01-29T18:07:13.134Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:08,245 | INFO  | aging/0-SNAP...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2021-01-29T18:07:13.134Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:05,257 | INFO  | ev.HealthMon...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>2021-01-29T18:07:13.134Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:07,956 | INFO  | b]-nio2-thre...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>2021-01-29T18:07:13.134Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:07,956 | INFO  | b]-nio2-thre...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>2021-01-29T18:07:13.134Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:08,245 | INFO  | aging/0-SNAP...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>2021-01-29T18:07:38.143Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:35,260 | INFO  | ev.HealthMon...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2021-01-29T18:07:38.143Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:07:35,260 | INFO  | ev.HealthMon...</td>\n",
       "      <td>nitf-messaging-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2021-01-29T18:08:20.167Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:08:17,955 | INFO  | scene/0-SNAP...</td>\n",
       "      <td>newscene-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>2021-01-29T18:09:10.178Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:09:05,267 | INFO  | ev.HealthMon...</td>\n",
       "      <td>newscene-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>2021-01-29T18:08:20.167Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:08:17,613 | INFO  | b]-nio2-thre...</td>\n",
       "      <td>newscene-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>2021-01-29T18:08:45.172Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:08:35,266 | INFO  | ev.HealthMon...</td>\n",
       "      <td>newscene-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>2021-01-29T18:08:45.172Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:08:35,266 | INFO  | ev.HealthMon...</td>\n",
       "      <td>newscene-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>2021-01-29T18:09:10.178Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:09:05,267 | INFO  | ev.HealthMon...</td>\n",
       "      <td>newscene-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2021-01-29T18:08:20.167Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:08:17,613 | INFO  | b]-nio2-thre...</td>\n",
       "      <td>newscene-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>2021-01-29T18:08:20.167Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:08:17,955 | INFO  | scene/0-SNAP...</td>\n",
       "      <td>newscene-bundle-stopped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>2021-01-29T18:09:25.185Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:09:21,229 | INFO  | scene/0-SNAP...</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>2021-01-29T18:09:25.185Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:09:20,828 | INFO  | b]-nio2-thre...</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2021-01-29T18:09:25.185Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:09:21,229 | INFO  | scene/0-SNAP...</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>2021-01-29T18:09:25.185Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:09:20,828 | INFO  | b]-nio2-thre...</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>2021-01-29T18:10:35.196Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:10:33,357 | INFO  | Framework st...</td>\n",
       "      <td>core.soaesb-dead-soa-process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>2021-01-29T18:10:35.196Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:10:33,367 | INFO  | Framework st...</td>\n",
       "      <td>core.soaesb-dead-soa-process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>2021-01-29T18:10:35.196Z</td>\n",
       "      <td>core.soaesb</td>\n",
       "      <td>2021-01-29T18:10:33,394 | INFO  | Framework st...</td>\n",
       "      <td>core.soaesb-dead-soa-process</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp container_name  \\\n",
       "9     2021-01-29T18:07:13.134Z    core.soaesb   \n",
       "64    2021-01-29T18:07:13.134Z    core.soaesb   \n",
       "65    2021-01-29T18:07:13.134Z    core.soaesb   \n",
       "182   2021-01-29T18:07:13.134Z    core.soaesb   \n",
       "369   2021-01-29T18:07:13.134Z    core.soaesb   \n",
       "370   2021-01-29T18:07:13.134Z    core.soaesb   \n",
       "435   2021-01-29T18:07:13.134Z    core.soaesb   \n",
       "487   2021-01-29T18:07:13.134Z    core.soaesb   \n",
       "537   2021-01-29T18:07:38.143Z    core.soaesb   \n",
       "589   2021-01-29T18:07:38.143Z    core.soaesb   \n",
       "630   2021-01-29T18:08:20.167Z    core.soaesb   \n",
       "675   2021-01-29T18:09:10.178Z    core.soaesb   \n",
       "687   2021-01-29T18:08:20.167Z    core.soaesb   \n",
       "714   2021-01-29T18:08:45.172Z    core.soaesb   \n",
       "784   2021-01-29T18:08:45.172Z    core.soaesb   \n",
       "970   2021-01-29T18:09:10.178Z    core.soaesb   \n",
       "997   2021-01-29T18:08:20.167Z    core.soaesb   \n",
       "1180  2021-01-29T18:08:20.167Z    core.soaesb   \n",
       "1226  2021-01-29T18:09:25.185Z    core.soaesb   \n",
       "1290  2021-01-29T18:09:25.185Z    core.soaesb   \n",
       "1442  2021-01-29T18:09:25.185Z    core.soaesb   \n",
       "1514  2021-01-29T18:09:25.185Z    core.soaesb   \n",
       "1547  2021-01-29T18:10:35.196Z    core.soaesb   \n",
       "1548  2021-01-29T18:10:35.196Z    core.soaesb   \n",
       "1549  2021-01-29T18:10:35.196Z    core.soaesb   \n",
       "\n",
       "                                                    log  \\\n",
       "9     2021-01-29T18:07:08,402 | INFO  | aging/0-SNAP...   \n",
       "64    2021-01-29T18:07:08,402 | INFO  | aging/0-SNAP...   \n",
       "65    2021-01-29T18:07:05,257 | INFO  | ev.HealthMon...   \n",
       "182   2021-01-29T18:07:08,245 | INFO  | aging/0-SNAP...   \n",
       "369   2021-01-29T18:07:05,257 | INFO  | ev.HealthMon...   \n",
       "370   2021-01-29T18:07:07,956 | INFO  | b]-nio2-thre...   \n",
       "435   2021-01-29T18:07:07,956 | INFO  | b]-nio2-thre...   \n",
       "487   2021-01-29T18:07:08,245 | INFO  | aging/0-SNAP...   \n",
       "537   2021-01-29T18:07:35,260 | INFO  | ev.HealthMon...   \n",
       "589   2021-01-29T18:07:35,260 | INFO  | ev.HealthMon...   \n",
       "630   2021-01-29T18:08:17,955 | INFO  | scene/0-SNAP...   \n",
       "675   2021-01-29T18:09:05,267 | INFO  | ev.HealthMon...   \n",
       "687   2021-01-29T18:08:17,613 | INFO  | b]-nio2-thre...   \n",
       "714   2021-01-29T18:08:35,266 | INFO  | ev.HealthMon...   \n",
       "784   2021-01-29T18:08:35,266 | INFO  | ev.HealthMon...   \n",
       "970   2021-01-29T18:09:05,267 | INFO  | ev.HealthMon...   \n",
       "997   2021-01-29T18:08:17,613 | INFO  | b]-nio2-thre...   \n",
       "1180  2021-01-29T18:08:17,955 | INFO  | scene/0-SNAP...   \n",
       "1226  2021-01-29T18:09:21,229 | INFO  | scene/0-SNAP...   \n",
       "1290  2021-01-29T18:09:20,828 | INFO  | b]-nio2-thre...   \n",
       "1442  2021-01-29T18:09:21,229 | INFO  | scene/0-SNAP...   \n",
       "1514  2021-01-29T18:09:20,828 | INFO  | b]-nio2-thre...   \n",
       "1547  2021-01-29T18:10:33,357 | INFO  | Framework st...   \n",
       "1548  2021-01-29T18:10:33,367 | INFO  | Framework st...   \n",
       "1549  2021-01-29T18:10:33,394 | INFO  | Framework st...   \n",
       "\n",
       "                              label  \n",
       "9     nitf-messaging-bundle-stopped  \n",
       "64    nitf-messaging-bundle-stopped  \n",
       "65    nitf-messaging-bundle-stopped  \n",
       "182   nitf-messaging-bundle-stopped  \n",
       "369   nitf-messaging-bundle-stopped  \n",
       "370   nitf-messaging-bundle-stopped  \n",
       "435   nitf-messaging-bundle-stopped  \n",
       "487   nitf-messaging-bundle-stopped  \n",
       "537   nitf-messaging-bundle-stopped  \n",
       "589   nitf-messaging-bundle-stopped  \n",
       "630         newscene-bundle-stopped  \n",
       "675         newscene-bundle-stopped  \n",
       "687         newscene-bundle-stopped  \n",
       "714         newscene-bundle-stopped  \n",
       "784         newscene-bundle-stopped  \n",
       "970         newscene-bundle-stopped  \n",
       "997         newscene-bundle-stopped  \n",
       "1180        newscene-bundle-stopped  \n",
       "1226                        healthy  \n",
       "1290                        healthy  \n",
       "1442                        healthy  \n",
       "1514                        healthy  \n",
       "1547   core.soaesb-dead-soa-process  \n",
       "1548   core.soaesb-dead-soa-process  \n",
       "1549   core.soaesb-dead-soa-process  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container_dataset.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiNfWpckbW3A"
   },
   "source": [
    "## W2V Pipeline Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ll4n92B5rGYW",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-12 17:51:24,707 INFO | Starting Drain3 template miner\n",
      "2021-05-12 17:51:24,708 INFO | Loading configuration from drain3.ini\n",
      "2021-05-12 17:51:26,639 INFO | collecting all words and their counts\n",
      "2021-05-12 17:51:26,639 INFO | PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-0dac1ccb5393>:4: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-12 17:51:27,157 INFO | PROGRESS: at sentence #10000, processed 292290 words and 7920 word types\n",
      "2021-05-12 17:51:27,664 INFO | PROGRESS: at sentence #20000, processed 585850 words and 12008 word types\n",
      "2021-05-12 17:51:28,076 INFO | collected 16000 token types (unigram + bigrams) from a corpus of 827523 words and 28226 sentences\n",
      "2021-05-12 17:51:28,076 INFO | merged Phrases<16000 vocab, min_count=5, threshold=7, max_vocab_size=40000000>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28226/28226 [52:25<00:00,  8.97it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 2.0707 - accuracy: 0.4609\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 1.1787 - accuracy: 0.5999\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 1.0345 - accuracy: 0.6243\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.9526 - accuracy: 0.6429\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 5s 35ms/step - loss: 0.9010 - accuracy: 0.6550\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.8729 - accuracy: 0.6611\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.8559 - accuracy: 0.6649\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 5s 34ms/step - loss: 0.8446 - accuracy: 0.6671\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8367 - accuracy: 0.6688\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8309 - accuracy: 0.6699\n",
      "Epoch 11/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8267 - accuracy: 0.6705\n",
      "Epoch 12/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8233 - accuracy: 0.6711\n",
      "Epoch 13/200\n",
      "150/150 [==============================] - 5s 35ms/step - loss: 0.8207 - accuracy: 0.6715\n",
      "Epoch 14/200\n",
      "150/150 [==============================] - 7s 45ms/step - loss: 0.8186 - accuracy: 0.6719\n",
      "Epoch 15/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8169 - accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8155 - accuracy: 0.6725\n",
      "Epoch 17/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8143 - accuracy: 0.6726\n",
      "Epoch 18/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8132 - accuracy: 0.6726\n",
      "Epoch 19/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8123 - accuracy: 0.6727\n",
      "Epoch 20/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8116 - accuracy: 0.6729\n",
      "Epoch 21/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8109 - accuracy: 0.6730\n",
      "Epoch 22/200\n",
      "150/150 [==============================] - 7s 46ms/step - loss: 0.8103 - accuracy: 0.6730\n",
      "Epoch 23/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8097 - accuracy: 0.6731\n",
      "Epoch 24/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8093 - accuracy: 0.6731\n",
      "Epoch 25/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8088 - accuracy: 0.6730\n",
      "Epoch 26/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8084 - accuracy: 0.6731\n",
      "Epoch 27/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8081 - accuracy: 0.6731\n",
      "Epoch 28/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8078 - accuracy: 0.6731\n",
      "Epoch 29/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8075 - accuracy: 0.6731\n",
      "Epoch 30/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8072 - accuracy: 0.6732\n",
      "Epoch 31/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8069 - accuracy: 0.6732\n",
      "Epoch 32/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8067 - accuracy: 0.6732\n",
      "Epoch 33/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8065 - accuracy: 0.6732\n",
      "Epoch 34/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8063 - accuracy: 0.6732\n",
      "Epoch 35/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8061 - accuracy: 0.6732\n",
      "Epoch 36/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8060 - accuracy: 0.6732\n",
      "Epoch 37/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8058 - accuracy: 0.6731\n",
      "Epoch 38/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8057 - accuracy: 0.6732\n",
      "Epoch 39/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8055 - accuracy: 0.6732\n",
      "Epoch 40/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8054 - accuracy: 0.6731\n",
      "Epoch 41/200\n",
      "150/150 [==============================] - 6s 42ms/step - loss: 0.8053 - accuracy: 0.6731\n",
      "Epoch 42/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8052 - accuracy: 0.6732\n",
      "Epoch 43/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8050 - accuracy: 0.6732\n",
      "Epoch 44/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8049 - accuracy: 0.6732\n",
      "Epoch 45/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8049 - accuracy: 0.6732\n",
      "Epoch 46/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8048 - accuracy: 0.6732\n",
      "Epoch 47/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8047 - accuracy: 0.6733\n",
      "Epoch 48/200\n",
      "150/150 [==============================] - 7s 46ms/step - loss: 0.8046 - accuracy: 0.6734\n",
      "Epoch 49/200\n",
      "150/150 [==============================] - 6s 41ms/step - loss: 0.8045 - accuracy: 0.6733\n",
      "Epoch 50/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8044 - accuracy: 0.6733\n",
      "Epoch 51/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8044 - accuracy: 0.6733\n",
      "Epoch 52/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8043 - accuracy: 0.6733\n",
      "Epoch 53/200\n",
      "150/150 [==============================] - 7s 44ms/step - loss: 0.8042 - accuracy: 0.6732\n",
      "Epoch 54/200\n",
      "150/150 [==============================] - 7s 44ms/step - loss: 0.8042 - accuracy: 0.6732\n",
      "Epoch 55/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8041 - accuracy: 0.6732\n",
      "Epoch 56/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8041 - accuracy: 0.6732\n",
      "Epoch 57/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8040 - accuracy: 0.6732\n",
      "Epoch 58/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8039 - accuracy: 0.6732\n",
      "Epoch 59/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8039 - accuracy: 0.6732\n",
      "Epoch 60/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8038 - accuracy: 0.6732\n",
      "Epoch 61/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8038 - accuracy: 0.6732\n",
      "Epoch 62/200\n",
      "150/150 [==============================] - 7s 44ms/step - loss: 0.8038 - accuracy: 0.6732\n",
      "Epoch 63/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8037 - accuracy: 0.6732\n",
      "Epoch 64/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8037 - accuracy: 0.6732\n",
      "Epoch 65/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8036 - accuracy: 0.6732\n",
      "Epoch 66/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8036 - accuracy: 0.6732\n",
      "Epoch 67/200\n",
      "150/150 [==============================] - 5s 35ms/step - loss: 0.8035 - accuracy: 0.6732\n",
      "Epoch 68/200\n",
      "150/150 [==============================] - 5s 35ms/step - loss: 0.8035 - accuracy: 0.6732\n",
      "Epoch 69/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8035 - accuracy: 0.6731\n",
      "Epoch 70/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8035 - accuracy: 0.6731\n",
      "Epoch 71/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8034 - accuracy: 0.6731\n",
      "Epoch 72/200\n",
      "150/150 [==============================] - 6s 41ms/step - loss: 0.8034 - accuracy: 0.6732\n",
      "Epoch 73/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8033 - accuracy: 0.6731\n",
      "Epoch 74/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8033 - accuracy: 0.6732\n",
      "Epoch 75/200\n",
      "150/150 [==============================] - 6s 43ms/step - loss: 0.8033 - accuracy: 0.6731\n",
      "Epoch 76/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8033 - accuracy: 0.6732\n",
      "Epoch 77/200\n",
      "150/150 [==============================] - 6s 43ms/step - loss: 0.8032 - accuracy: 0.6731\n",
      "Epoch 78/200\n",
      "150/150 [==============================] - 6s 42ms/step - loss: 0.8032 - accuracy: 0.6732\n",
      "Epoch 79/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8032 - accuracy: 0.6731\n",
      "Epoch 80/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8032 - accuracy: 0.6732\n",
      "Epoch 81/200\n",
      "150/150 [==============================] - 5s 35ms/step - loss: 0.8031 - accuracy: 0.6732\n",
      "Epoch 82/200\n",
      "150/150 [==============================] - 5s 37ms/step - loss: 0.8031 - accuracy: 0.6732\n",
      "Epoch 83/200\n",
      "150/150 [==============================] - 7s 43ms/step - loss: 0.8031 - accuracy: 0.6731\n",
      "Epoch 84/200\n",
      "150/150 [==============================] - 8s 53ms/step - loss: 0.8030 - accuracy: 0.6732\n",
      "Epoch 85/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8030 - accuracy: 0.6732\n",
      "Epoch 86/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8030 - accuracy: 0.6732\n",
      "Epoch 87/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8030 - accuracy: 0.6731\n",
      "Epoch 88/200\n",
      "150/150 [==============================] - 7s 43ms/step - loss: 0.8030 - accuracy: 0.6731\n",
      "Epoch 89/200\n",
      "150/150 [==============================] - 6s 41ms/step - loss: 0.8029 - accuracy: 0.6731\n",
      "Epoch 90/200\n",
      "150/150 [==============================] - 5s 37ms/step - loss: 0.8029 - accuracy: 0.6731\n",
      "Epoch 91/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8029 - accuracy: 0.6731\n",
      "Epoch 92/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8029 - accuracy: 0.6731\n",
      "Epoch 93/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8028 - accuracy: 0.6731\n",
      "Epoch 94/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8028 - accuracy: 0.6731\n",
      "Epoch 95/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8028 - accuracy: 0.6731\n",
      "Epoch 96/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8028 - accuracy: 0.6732\n",
      "Epoch 97/200\n",
      "150/150 [==============================] - 5s 35ms/step - loss: 0.8028 - accuracy: 0.6731\n",
      "Epoch 98/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8028 - accuracy: 0.6731\n",
      "Epoch 99/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8027 - accuracy: 0.6731\n",
      "Epoch 100/200\n",
      "150/150 [==============================] - 7s 49ms/step - loss: 0.8027 - accuracy: 0.6731\n",
      "Epoch 101/200\n",
      "150/150 [==============================] - 8s 53ms/step - loss: 0.8027 - accuracy: 0.6732\n",
      "Epoch 102/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8027 - accuracy: 0.6732\n",
      "Epoch 103/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8027 - accuracy: 0.6731\n",
      "Epoch 104/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8027 - accuracy: 0.6732\n",
      "Epoch 105/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8026 - accuracy: 0.6731\n",
      "Epoch 106/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8026 - accuracy: 0.6731\n",
      "Epoch 107/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8026 - accuracy: 0.6731\n",
      "Epoch 108/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8026 - accuracy: 0.6732\n",
      "Epoch 109/200\n",
      "150/150 [==============================] - 6s 41ms/step - loss: 0.8026 - accuracy: 0.6731\n",
      "Epoch 110/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8026 - accuracy: 0.6732\n",
      "Epoch 111/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8025 - accuracy: 0.6731\n",
      "Epoch 112/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8025 - accuracy: 0.6732\n",
      "Epoch 113/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8025 - accuracy: 0.6731\n",
      "Epoch 114/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8025 - accuracy: 0.6731\n",
      "Epoch 115/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8025 - accuracy: 0.6731\n",
      "Epoch 116/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8025 - accuracy: 0.6731\n",
      "Epoch 117/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8024 - accuracy: 0.6731\n",
      "Epoch 118/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8024 - accuracy: 0.6731\n",
      "Epoch 119/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8024 - accuracy: 0.6731\n",
      "Epoch 120/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8024 - accuracy: 0.6731\n",
      "Epoch 121/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8024 - accuracy: 0.6732\n",
      "Epoch 122/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8024 - accuracy: 0.6731\n",
      "Epoch 123/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8024 - accuracy: 0.6732\n",
      "Epoch 124/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8024 - accuracy: 0.6732\n",
      "Epoch 125/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8023 - accuracy: 0.6732\n",
      "Epoch 126/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8023 - accuracy: 0.6731\n",
      "Epoch 127/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8023 - accuracy: 0.6732\n",
      "Epoch 128/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8023 - accuracy: 0.6732\n",
      "Epoch 129/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8023 - accuracy: 0.6732\n",
      "Epoch 130/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8023 - accuracy: 0.6731\n",
      "Epoch 131/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8023 - accuracy: 0.6732\n",
      "Epoch 132/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8023 - accuracy: 0.6732\n",
      "Epoch 133/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8022 - accuracy: 0.6732\n",
      "Epoch 134/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8022 - accuracy: 0.6732\n",
      "Epoch 135/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8022 - accuracy: 0.6732\n",
      "Epoch 136/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8022 - accuracy: 0.6732\n",
      "Epoch 137/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8022 - accuracy: 0.6732\n",
      "Epoch 138/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8022 - accuracy: 0.6732\n",
      "Epoch 139/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8022 - accuracy: 0.6732\n",
      "Epoch 140/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8022 - accuracy: 0.6731\n",
      "Epoch 141/200\n",
      "150/150 [==============================] - 6s 42ms/step - loss: 0.8021 - accuracy: 0.6731\n",
      "Epoch 142/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8021 - accuracy: 0.6731\n",
      "Epoch 143/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8021 - accuracy: 0.6732\n",
      "Epoch 144/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8021 - accuracy: 0.6732\n",
      "Epoch 145/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8021 - accuracy: 0.6732\n",
      "Epoch 146/200\n",
      "150/150 [==============================] - 6s 41ms/step - loss: 0.8021 - accuracy: 0.6732\n",
      "Epoch 147/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8021 - accuracy: 0.6732\n",
      "Epoch 148/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8021 - accuracy: 0.6732\n",
      "Epoch 149/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8021 - accuracy: 0.6732\n",
      "Epoch 150/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8021 - accuracy: 0.6732\n",
      "Epoch 151/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8020 - accuracy: 0.6732\n",
      "Epoch 152/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8020 - accuracy: 0.6732\n",
      "Epoch 153/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8020 - accuracy: 0.6732\n",
      "Epoch 154/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8020 - accuracy: 0.6732\n",
      "Epoch 155/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8020 - accuracy: 0.6732\n",
      "Epoch 156/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8020 - accuracy: 0.6732\n",
      "Epoch 157/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8020 - accuracy: 0.6732\n",
      "Epoch 158/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8020 - accuracy: 0.6732\n",
      "Epoch 159/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8019 - accuracy: 0.6732\n",
      "Epoch 160/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8019 - accuracy: 0.6733\n",
      "Epoch 161/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8019 - accuracy: 0.6733\n",
      "Epoch 162/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8019 - accuracy: 0.6733\n",
      "Epoch 163/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8019 - accuracy: 0.6733\n",
      "Epoch 164/200\n",
      "150/150 [==============================] - 5s 37ms/step - loss: 0.8019 - accuracy: 0.6733\n",
      "Epoch 165/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8019 - accuracy: 0.6733\n",
      "Epoch 166/200\n",
      "150/150 [==============================] - 5s 37ms/step - loss: 0.8019 - accuracy: 0.6733\n",
      "Epoch 167/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8019 - accuracy: 0.6732\n",
      "Epoch 168/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8019 - accuracy: 0.6732\n",
      "Epoch 169/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8018 - accuracy: 0.6732\n",
      "Epoch 170/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8018 - accuracy: 0.6732\n",
      "Epoch 171/200\n",
      "150/150 [==============================] - 7s 44ms/step - loss: 0.8018 - accuracy: 0.6732\n",
      "Epoch 172/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8018 - accuracy: 0.6732\n",
      "Epoch 173/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8018 - accuracy: 0.6733\n",
      "Epoch 174/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8018 - accuracy: 0.6733\n",
      "Epoch 175/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8018 - accuracy: 0.6733\n",
      "Epoch 176/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8018 - accuracy: 0.6733\n",
      "Epoch 177/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8018 - accuracy: 0.6733\n",
      "Epoch 178/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8018 - accuracy: 0.6733\n",
      "Epoch 179/200\n",
      "150/150 [==============================] - 6s 42ms/step - loss: 0.8018 - accuracy: 0.6733\n",
      "Epoch 180/200\n",
      "150/150 [==============================] - 7s 44ms/step - loss: 0.8018 - accuracy: 0.6733\n",
      "Epoch 181/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 182/200\n",
      "150/150 [==============================] - 8s 51ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 183/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 184/200\n",
      "150/150 [==============================] - 6s 40ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 185/200\n",
      "150/150 [==============================] - 6s 41ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 186/200\n",
      "150/150 [==============================] - 7s 47ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 187/200\n",
      "150/150 [==============================] - 7s 47ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 188/200\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 189/200\n",
      "150/150 [==============================] - 6s 42ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 190/200\n",
      "150/150 [==============================] - 7s 49ms/step - loss: 0.8017 - accuracy: 0.6733\n",
      "Epoch 191/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8016 - accuracy: 0.6733\n",
      "Epoch 192/200\n",
      "150/150 [==============================] - 5s 37ms/step - loss: 0.8016 - accuracy: 0.6733\n",
      "Epoch 193/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8016 - accuracy: 0.6733\n",
      "Epoch 194/200\n",
      "150/150 [==============================] - 6s 38ms/step - loss: 0.8016 - accuracy: 0.6733\n",
      "Epoch 195/200\n",
      "150/150 [==============================] - 5s 37ms/step - loss: 0.8016 - accuracy: 0.6733\n",
      "Epoch 196/200\n",
      "150/150 [==============================] - 6s 39ms/step - loss: 0.8016 - accuracy: 0.6733\n",
      "Epoch 197/200\n",
      "150/150 [==============================] - 6s 42ms/step - loss: 0.8016 - accuracy: 0.6733\n",
      "Epoch 198/200\n",
      "150/150 [==============================] - 5s 37ms/step - loss: 0.8016 - accuracy: 0.6734\n",
      "Epoch 199/200\n",
      "150/150 [==============================] - 6s 37ms/step - loss: 0.8016 - accuracy: 0.6733\n",
      "Epoch 200/200\n",
      "150/150 [==============================] - 6s 41ms/step - loss: 0.8016 - accuracy: 0.6734\n",
      "INFO:tensorflow:Assets written to: /home/the-optimal-policies/app/results/word2vec/assets\n",
      "2021-05-12 19:06:18,133 INFO | Assets written to: /home/the-optimal-policies/app/results/word2vec/assets\n",
      "Model: \"word2_vec_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "w2v_embedding (Embedding)    multiple                  348672    \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      multiple                  348672    \n",
      "_________________________________________________________________\n",
      "dot_1 (Dot)                  multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 697,344\n",
      "Trainable params: 697,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ** Preprocessing **\n",
    "'''\n",
    "standardize_logs\n",
    "'''\n",
    "\n",
    "# ** Model **\n",
    "# 1.\n",
    "# LogTokenEmbedder\n",
    "'''\n",
    "Seq = [PCL\n",
    "       TCL\n",
    "       NSL\n",
    "       GT1: W2V] -> {embedding_matrix, vocab}\n",
    "'''\n",
    "######\n",
    "\n",
    "# 2.\n",
    "# Transformer Stuff\n",
    "'''\n",
    "{log, embedding_matrix, vocab} ->\n",
    "GT2: Transformer -> prediction\n",
    "'''\n",
    "# LOG_DIR = SOURCE + 'logs'\n",
    "# metadata = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "# config = projector.ProjectorConfig()\n",
    "\n",
    "log_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "w2vp = W2V_Pipeline(load_model=False, save_model=True)\n",
    "embed_weights = w2vp(container_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWSpd9MecA8b"
   },
   "source": [
    "## W2V Dash "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRGlsN0McF1D"
   },
   "source": [
    "### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "l9AAeUOmnz4h"
   },
   "outputs": [],
   "source": [
    "def tree_parser(node, inner_list, outer_list, root_node, depth):\n",
    "    d = node.key_to_child_node  # dict\n",
    "    for token in list(d.keys()):\n",
    "        if len(root_node.key_to_child_node.keys()) == 0:\n",
    "            ret_list = []\n",
    "            for row in outer_list:\n",
    "                proper_len = int(row[1])\n",
    "                if len(row) == proper_len+1 or len(row) + 1 == depth:\n",
    "                    ret_list.append(row)\n",
    "            return ret_list\n",
    "        inner_list.append(token)\n",
    "        child = d[token]\n",
    "        if child.key_to_child_node:\n",
    "            tree_parser(child, inner_list, outer_list, root_node, depth)\n",
    "        else:\n",
    "            d.pop(token)\n",
    "            outer_list.append(inner_list)\n",
    "            inner_list = ['root']\n",
    "            tree_parser(root_node, inner_list, outer_list, root_node, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_list_parser(node):\n",
    "    tree_df = []\n",
    "    curr_path = []\n",
    "    tree_dict = {}\n",
    "    prev_root = [(\"root\", node)]\n",
    "    while len(prev_root) > 0:\n",
    "        # Peek at last value\n",
    "        curr_root = prev_root[-1]\n",
    "\n",
    "        # Get the node element\n",
    "        curr_node = curr_root[1].key_to_child_node\n",
    "\n",
    "        # Follow path value if not already there\n",
    "        if len(curr_path) <= 0 or curr_path[-1] != curr_root[0]:\n",
    "            curr_path.append(curr_root[0])\n",
    "\n",
    "        visited = False\n",
    "        if curr_root[1] in tree_dict:\n",
    "            visited = True\n",
    "        else:\n",
    "            tree_dict[curr_root[1]] = True\n",
    "\n",
    "        # Check if value has any leaf nodes\n",
    "        if not visited and len(curr_node.keys()) > 0:\n",
    "            # Add those to the stack\n",
    "            for nn in curr_node.items():\n",
    "                prev_root.append((nn[0], nn[1]))\n",
    "        else:\n",
    "            # Remove previous node in the path\n",
    "            prev_root.pop()\n",
    "\n",
    "            # Record to the database if leaf\n",
    "            if len(curr_node.keys()) <= 0:\n",
    "                tree_df.append(deepcopy(curr_path))\n",
    "\n",
    "            # Move back up tree\n",
    "            curr_path.pop()\n",
    "    return tree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendSpherical_np(xyz):\n",
    "    ptsnew = np.hstack((xyz, np.zeros(xyz.shape)))\n",
    "    xy = xyz[:, 0]**2 + xyz[:, 1]**2\n",
    "    ptsnew[:, 3] = np.sqrt(xy + xyz[:, 2]**2)\n",
    "    ptsnew[:, 4] = np.arctan2(np.sqrt(xy), xyz[:, 2])  # for elevation angle defined from Z-axis down\n",
    "    # ptsnew[:,4] = np.arctan2(xyz[:,2], np.sqrt(xy)) # for elevation angle defined from XY-plane up\n",
    "    ptsnew[:, 5] = np.arctan2(xyz[:, 1], xyz[:, 0])\n",
    "    return ptsnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spherical_coords(xyz):\n",
    "    sph = np.zeros(shape=xyz.shape)\n",
    "    xy = xyz[:, 0]**2 + xyz[:, 1]**2\n",
    "    sph[:, 0] = np.sqrt(xy + xyz[:, 2]**2)\n",
    "    sph[:, 1] = np.arctan2(np.sqrt(xy), xyz[:, 2])\n",
    "    sph[:, 2] = np.arctan2(xyz[:, 1], xyz[:, 0])\n",
    "    return sph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCgMbFevmC88"
   },
   "source": [
    "The output of the W2V pipeline is a matrix of size [vocab size x embedding size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "jw4mwJz6eGa6"
   },
   "outputs": [],
   "source": [
    "# -- W2V Dash Environmental Variables -- #\n",
    "\n",
    "W2V_NEIGHBORS = 20\n",
    "RECURSION_LIMIT = 10**6\n",
    "N_PROJ_DIM = 3\n",
    "DASH_SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Projection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "a60tfcLUoERG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.8/site-packages/sklearn/decomposition/_fastica.py:118: ConvergenceWarning:\n",
      "\n",
      "FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Generate Data for Word Embeddings Projector -- #\n",
    "\n",
    "# shape = vocab size x embedding dim size\n",
    "weights = np.ndarray(shape=(len(embed_weights), W2V_EMBED_SIZE))\n",
    "\n",
    "# -- Populate Matrix for PCA -- #\n",
    "for idx, weight in enumerate(list(embed_weights.values())):\n",
    "    weights[idx, :] = weight\n",
    "\n",
    "# -- Dimensionality Reduction -- #\n",
    "pca = PCA(n_components=N_PROJ_DIM, random_state=DASH_SEED).fit(weights)\n",
    "ica = FastICA(n_components=N_PROJ_DIM, random_state=DASH_SEED).fit(weights)\n",
    "srp = SparseRandomProjection(n_components=N_PROJ_DIM, random_state=DASH_SEED).fit(weights)\n",
    "reduced_embeddings = pca.transform(weights)\n",
    "\n",
    "# -- Calculate Nearest Neighbors -- #\n",
    "model = NearestNeighbors(n_neighbors=W2V_NEIGHBORS, algorithm='auto')\n",
    "trained_embeddings = model.fit(reduced_embeddings)\n",
    "\n",
    "# Currently the array has a shape of vocab size x N_PROJ_DIM and contains\n",
    "# the fitted PCA data. We need to add the vocab in the first column so\n",
    "# we know which vectors are represented.\n",
    "scatter_plot_3d_cols = ['token', 'x1', 'x2', 'x3']\n",
    "embedding_vocab_arr = np.array(list(embed_weights.keys()))\n",
    "embedding_vocab_arr = np.expand_dims(embedding_vocab_arr, 1)\n",
    "named_reduced_embeddings = np.hstack((embedding_vocab_arr, reduced_embeddings))\n",
    "scatter_plot_3d_df = pd.DataFrame(\n",
    "    data=named_reduced_embeddings,\n",
    "    columns=scatter_plot_3d_cols)\n",
    "scatter_plot_3d_df['x1'] = pd.to_numeric(scatter_plot_3d_df['x1'])\n",
    "scatter_plot_3d_df['x2'] = pd.to_numeric(scatter_plot_3d_df['x2'])\n",
    "scatter_plot_3d_df['x3'] = pd.to_numeric(scatter_plot_3d_df['x3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC17hivjjKs0"
   },
   "source": [
    "We will build our plot using the tree_parser function. This function recursively\n",
    "steps through the drain3.TemplateMiner.drain.Node structure of our \n",
    "**TextClusteringLayer** (TCL). The recursion populates a np.array which is then used\n",
    "to build a pandas dataframe which the plotly treemap accepts. There is a column\n",
    "appended to the tail of the dataframe which counts the number of stars \n",
    "(wild card masks) present in the row. This is used to define the colors shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Treemap Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "X7TkKEOFyRH3"
   },
   "outputs": [],
   "source": [
    "# By default python's recursion limit is 10**4 which is too small for our needs\n",
    "sys.setrecursionlimit(RECURSION_LIMIT)\n",
    "\n",
    "# The root node is the master node of the tree and will be our return point\n",
    "root_node = deepcopy(w2vp.TCL.template_miner.drain.root_node)\n",
    "parsed_tree = tree_to_list_parser(root_node)\n",
    "parsed_tree_df = pd.DataFrame(data=parsed_tree)\n",
    "\n",
    "# The returned dataframe has generic columns so we will provide custom labels\n",
    "n_cols = len(parsed_tree_df.columns)\n",
    "col_name_list = []\n",
    "for idx in range(n_cols):\n",
    "    col_name_list.append('level' + str(idx))\n",
    "parsed_tree_df.columns = col_name_list\n",
    "\n",
    "'''\n",
    "Without a color column our treemap would just be plain. We thought that taking\n",
    "the sum of the drain mask would be an interesting way to color the treemap.\n",
    "This lambda function will sum those values in each row and return them to a new\n",
    "columnn named 'sum'\n",
    "'''\n",
    "parsed_tree_df['sum'] = parsed_tree_df.apply(lambda x: x.str.contains('<*>'), axis=1).sum(axis=1)  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dash Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates.default = \"plotly_dark\"\n",
    "external_stylesheets_url = 'https://drive.google.com/uc?export=view&id=19OXGQ5iJIjRZD4VEZ-xiVChDmj0-SlSF'  # noqa\n",
    "external_stylesheets = [external_stylesheets_url]\n",
    "\n",
    "CACHE_CONFIG = dict()\n",
    "CACHE_CONFIG['CACHE_TYPE'] = 'filesystem'\n",
    "CACHE_CONFIG['CACHE_DIR'] = SOURCE + '/results/dash_cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_d = dict()\n",
    "color_d['blue'] = 'rgb(66, 133, 244)'\n",
    "color_d['red'] = 'rgb(219, 68, 55)'\n",
    "color_d['yellow'] = 'rgb(244, 180, 0)'\n",
    "color_d['orange'] = 'rgb(255, 165, 0)'\n",
    "color_d['green'] = 'rgb(15, 157, 88)'\n",
    "color_d['mint'] = 'rgb(3, 218, 198)'\n",
    "color_d['dark mint'] = 'rgb(1, 135, 134)'\n",
    "color_d['dark purple'] = 'rgb(55, 0, 179)'\n",
    "color_d['purple'] = 'rgb(98, 0, 238)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dash Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= #\n",
    "#  3d Scatter Plot  #\n",
    "# ================= #\n",
    "\n",
    "# Line formatting\n",
    "scatter_plot_3d_line = dict()\n",
    "scatter_plot_3d_line['width'] = 2\n",
    "scatter_plot_3d_line['color'] = color_d['dark mint']\n",
    "\n",
    "scatter_plot_3d_selected_line = dict()\n",
    "scatter_plot_3d_selected_line['width'] = 2\n",
    "scatter_plot_3d_selected_line['color'] = color_d['dark mint']\n",
    "\n",
    "scatter_plot_3d_nonselected_line = dict()\n",
    "scatter_plot_3d_nonselected_line['width'] = 2\n",
    "scatter_plot_3d_nonselected_line['color'] = color_d['dark mint']\n",
    "\n",
    "scatter_ploy_3d_darker_line = dict()\n",
    "scatter_ploy_3d_darker_line['width'] = 2\n",
    "scatter_ploy_3d_darker_line['color'] = color_d['dark purple']\n",
    "\n",
    "\n",
    "# Marker formatting\n",
    "scatter_plot_3d_marker = dict()\n",
    "scatter_plot_3d_marker['size'] = 5\n",
    "scatter_plot_3d_marker['line'] = scatter_plot_3d_line\n",
    "scatter_plot_3d_marker['color'] = color_d['mint']\n",
    "\n",
    "scatter_plot_3d_selected_marker = dict()\n",
    "scatter_plot_3d_selected_marker['size'] = 5\n",
    "scatter_plot_3d_selected_marker['color'] = color_d['mint']\n",
    "scatter_plot_3d_selected_marker['line'] = scatter_plot_3d_selected_line\n",
    "\n",
    "scatter_plot_3d_nonselected_marker = dict()\n",
    "scatter_plot_3d_nonselected_marker['size'] = 5\n",
    "scatter_plot_3d_nonselected_marker['color'] = color_d['mint']\n",
    "scatter_plot_3d_nonselected_marker['opacity'] = 0.15\n",
    "scatter_plot_3d_nonselected_marker['line'] = scatter_plot_3d_nonselected_line\n",
    "\n",
    "scatter_plot_3d_marker_no_color = dict()\n",
    "scatter_plot_3d_marker_no_color['size'] = 5\n",
    "scatter_plot_3d_marker_no_color['line'] = scatter_ploy_3d_darker_line\n",
    "\n",
    "scatter_plot_3d_marker_cluster_center = dict()\n",
    "scatter_plot_3d_marker_cluster_center['size'] = 10\n",
    "scatter_plot_3d_marker_cluster_center['color'] = color_d['orange']\n",
    "scatter_plot_3d_marker_cluster_center['opacity'] = 0.5\n",
    "scatter_plot_3d_marker_cluster_center['line'] = scatter_ploy_3d_darker_line\n",
    "\n",
    "\n",
    "\n",
    "# Style\n",
    "scatter_plot_3d_style = dict()\n",
    "scatter_plot_3d_style['height'] = '100%'\n",
    "scatter_plot_3d_style['width'] = '100%'\n",
    "\n",
    "\n",
    "# ========= #\n",
    "#  Treemap  #\n",
    "# ========= #\n",
    "\n",
    "# Style\n",
    "treemap_style = dict()\n",
    "treemap_style['height'] = '100%'\n",
    "treemap_style['width'] = '100%'\n",
    "\n",
    "\n",
    "# ============ #\n",
    "#  Data Table  #\n",
    "# ============ #\n",
    "\n",
    "# Style\n",
    "data_table_cell_style = dict()\n",
    "data_table_cell_style['textAlign'] = 'left'\n",
    "data_table_cell_style['overflow'] = 'hidden'\n",
    "data_table_cell_style['textOverflow'] = 'ellipsis'\n",
    "data_table_cell_style['maxWidth'] = 0\n",
    "data_table_cell_style['backgroundColor'] = 'rgb(20, 20, 20)'\n",
    "data_table_cell_style['color'] = 'white'\n",
    "\n",
    "data_table_header_style = dict()\n",
    "data_table_header_style['backgroundColor'] = color_d['purple']\n",
    "\n",
    "\n",
    "# ======== #\n",
    "#  Labels  #\n",
    "# ======== #\n",
    "\n",
    "# Style\n",
    "clustering_alg_drop_down_label_style = dict()\n",
    "clustering_alg_drop_down_label_style['color'] = 'white'\n",
    "\n",
    "coordinate_space_drop_down_label_style = dict()\n",
    "coordinate_space_drop_down_label_style['color'] = 'white'\n",
    "\n",
    "dim_reduction_drop_down_label_style = dict()\n",
    "dim_reduction_drop_down_label_style['color'] = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dash Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= #\n",
    "#  3d Scatter Plot  #\n",
    "# ================= #\n",
    "scatter_plot_3d_config = dict()\n",
    "scatter_plot_3d_config['responsive'] = True\n",
    "\n",
    "\n",
    "# ========= #\n",
    "#  Treemap  #\n",
    "# ========= #\n",
    "treemap_config = dict()\n",
    "treemap_config['responsive'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dash Dropdown Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_alg_drop_down_options = [\n",
    "    {'label': 'KNN', 'value': 'KNN'},\n",
    "    {'label': 'GMM', 'value': 'GMM'},\n",
    "    {'label': 'Bayesian GMM', 'value': 'BGMM'},\n",
    "    {'label': 'Affinity Prop.', 'value': 'AP'},\n",
    "    {'label': 'KMEANS', 'value': 'KM'},\n",
    "    {'label': 'SVM', 'value': 'SVM'},\n",
    "]\n",
    "\n",
    "coordinate_space_drop_down_options = [\n",
    "    {'label': 'Cartesian', 'value': 'CT'},\n",
    "    {'label': 'Spherical', 'value': 'SP'}\n",
    "]\n",
    "\n",
    "dim_reduction_drop_down_options = [\n",
    "    {'label': 'PCA', 'value': 'PCA'},\n",
    "    {'label': 'ICA', 'value': 'ICA'},\n",
    "    {'label': 'LDA', 'value': 'LDA'},\n",
    "    {'label': 'Sparse RP', 'value': 'SRP'},\n",
    "    {'label': 'Gaussian RP', 'value': 'GRP'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dash Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Je-dqJAGlzA9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.8/site-packages/sklearn/cluster/_affinity_propagation.py:246: ConvergenceWarning:\n",
      "\n",
      "Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "\n",
      "/env/lib/python3.8/site-packages/sklearn/cluster/_affinity_propagation.py:460: ConvergenceWarning:\n",
      "\n",
      "This model does not have any cluster centers because affinity propagation did not converge. Labeling every sample as '-1'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
    "cache = Cache()\n",
    "cache.init_app(app.server, config=CACHE_CONFIG)\n",
    "\n",
    "\n",
    "# =============== #\n",
    "#  Cluster Table  #\n",
    "# =============== #\n",
    "table = pd.DataFrame(\n",
    "    data=list(embed_weights.keys()),\n",
    "    columns=['token'])\n",
    "\n",
    "# ============= #\n",
    "#  Scatterplot  #\n",
    "# ============= #\n",
    "scatter_plot_3d_fig = px.scatter_3d(\n",
    "                      scatter_plot_3d_df,\n",
    "                      x='x1',\n",
    "                      y='x2',\n",
    "                      z='x3',\n",
    "                      hover_name='token')\n",
    "\n",
    "scatter_plot_3d_fig.update_traces(marker=scatter_plot_3d_marker)\n",
    "scatter_plot_3d_fig['layout']['uirevision'] = 1\n",
    "\n",
    "\n",
    "# ========= #\n",
    "#  Treemap  #\n",
    "# ========= #\n",
    "treemap_fig = px.treemap(\n",
    "    parsed_tree_df,\n",
    "    path=col_name_list,\n",
    "    color='sum')\n",
    "\n",
    "\n",
    "# ============ #\n",
    "#  App Layout  #\n",
    "# ============ #\n",
    "app.layout = html.Div([\n",
    "\n",
    "        html.Div([\n",
    "\n",
    "            # -- Clustering Technique Dropdown -- #\n",
    "            html.Label(\n",
    "                \"Clustering Algorithm (TODO)\",\n",
    "                style=clustering_alg_drop_down_label_style),\n",
    "            dcc.Dropdown(\n",
    "                id='cluster-dropdown',\n",
    "                options=clustering_alg_drop_down_options,\n",
    "                value='KNN'),\n",
    "\n",
    "            # -- Coordinate Space Dropdown -- #\n",
    "            html.Label(\n",
    "                \"Coordinate Space\",\n",
    "                style=coordinate_space_drop_down_label_style),\n",
    "            dcc.Dropdown(\n",
    "                id='coord-dropdown',\n",
    "                options=coordinate_space_drop_down_options,\n",
    "                value='CT'),\n",
    "\n",
    "            # -- Dimensionality Reduction Technique Dropdown -- #\n",
    "            html.Label(\n",
    "                \"Dimensionality Reduction (TODO)\",\n",
    "                style=dim_reduction_drop_down_label_style),\n",
    "            dcc.Dropdown(\n",
    "                id='dr-dropdown',\n",
    "                options=dim_reduction_drop_down_options,\n",
    "                value='PCA'\n",
    "            )\n",
    "        ], className='options-graph-container'),\n",
    "\n",
    "        # -- 3d Scatter Plot -- #\n",
    "        html.Div(\n",
    "            [dcc.Graph(\n",
    "                id='3d_scat',\n",
    "                figure=scatter_plot_3d_fig,\n",
    "                config=scatter_plot_3d_config,\n",
    "                style=scatter_plot_3d_style),\n",
    "             dcc.Slider(\n",
    "                id='my-slider',\n",
    "                min=0.5,\n",
    "                max=0.9,\n",
    "                step=0.05,\n",
    "                value=0.5)],\n",
    "            className='main-graph-container',\n",
    "            id='graph_div'),\n",
    "\n",
    "        # -- Tree Map -- #\n",
    "        html.Div(\n",
    "            dcc.Graph(\n",
    "                id='3d_tree',\n",
    "                figure=treemap_fig,\n",
    "                config=treemap_config,\n",
    "                style=treemap_style),\n",
    "            className='secondary-graph-container',\n",
    "            id='tree_div'),\n",
    "\n",
    "        # -- Neighbors Datatable -- #\n",
    "        html.Div(\n",
    "            children=[dash_table.DataTable(\n",
    "                 id='table',\n",
    "                 columns=[{\"name\": i, \"id\": i} for i in table.columns],\n",
    "                 data=pd.DataFrame().to_dict('records'),\n",
    "                 style_cell=data_table_cell_style,\n",
    "                 style_header=data_table_header_style,\n",
    "             )],\n",
    "            className='related-graph',\n",
    "            id='data_table'),\n",
    "\n",
    "        # signal value to trigger callbacks\n",
    "        dcc.Store(id='signal')],\n",
    "\n",
    "    id='report-container')\n",
    "\n",
    "\n",
    "# ============= #\n",
    "#  Memoization  #\n",
    "# ============= #\n",
    "\n",
    "# Table of Contents:\n",
    "# -----------------------------\n",
    "# 1. Projection DataFrame\n",
    "# 2. Coordinates\n",
    "# 3. Dimensionality Reductions\n",
    "# 4. Clustering Algorithms\n",
    "# -----------------------------\n",
    "\n",
    "# -- 1. Projection DataFrame -- #\n",
    "@cache.memoize()\n",
    "def dataframe_store(embeddings):\n",
    "    new_df = pd.DataFrame(\n",
    "        data=embeddings,\n",
    "        columns=scatter_plot_3d_cols)\n",
    "    new_df['x1'] = pd.to_numeric(new_df['x1'])\n",
    "    new_df['x2'] = pd.to_numeric(new_df['x2'])\n",
    "    new_df['x3'] = pd.to_numeric(new_df['x3'])\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# -- 2. Coordinates -- #\n",
    "@cache.memoize()\n",
    "def coordinate_space_store(value, embeddings):\n",
    "    # calculate new coordinate space\n",
    "    if value == 'SP':\n",
    "        spherical_embeddings = get_spherical_coords(embeddings)\n",
    "        embeddings_stack_tup = (embedding_vocab_arr, spherical_embeddings)\n",
    "        named_embeddings = np.hstack(embeddings_stack_tup)\n",
    "    elif value == \"CT\":\n",
    "        embeddings_stack_tup = (embedding_vocab_arr, embeddings)\n",
    "        named_embeddings = np.hstack(embeddings_stack_tup)\n",
    "    else:\n",
    "        return no_update\n",
    "    return named_embeddings\n",
    "\n",
    "\n",
    "# -- 3. Dimensionality Reduction -- #\n",
    "@cache.memoize()\n",
    "def dimension_reduct_store(value):\n",
    "    # calculate new dimensionality reduction algorithm\n",
    "    if value == \"PCA\":\n",
    "        dr_embeddings = pca.transform(weights)\n",
    "    elif value == \"ICA\":\n",
    "        dr_embeddings = ica.transform(weights)\n",
    "    elif value == \"SRP\":\n",
    "        dr_embeddings = srp.transform(weights)\n",
    "    else:\n",
    "        return no_update\n",
    "    return dr_embeddings\n",
    "\n",
    "\n",
    "# -- 4. Clustering Algorithms -- #\n",
    "@cache.memoize()\n",
    "def clustering_algo_store(value, damp_value):\n",
    "    # calculate new clustering algorithm\n",
    "    if value == \"KNN\":\n",
    "        model = NearestNeighbors(n_neighbors=W2V_NEIGHBORS, algorithm='auto')\n",
    "    elif value == \"AP\":\n",
    "        model = AffinityPropagation(damping=damp_value, random_state=DASH_SEED)\n",
    "    elif value == \"KM\":\n",
    "        model = KMeans(n_clusters=4)\n",
    "    elif value == \"GMM\":\n",
    "        model = GaussianMixture(n_components=4)\n",
    "    elif value == \"SVM\":\n",
    "        model = SVC(kernel='poly', degree=3, probability=True, random_state=DASH_SEED)\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========== #\n",
    "#  Callbacks  #\n",
    "# =========== #\n",
    "\n",
    "# -- Calculate Projection Data -- #\n",
    "@app.callback(Output('signal', 'data'),\n",
    "              Input('dr-dropdown', 'value'),\n",
    "              Input('cluster-dropdown', 'value'),\n",
    "              Input('coord-dropdown', 'value'),\n",
    "              Input('my-slider', 'value'))\n",
    "def compute_coordinate_space(dr_val, cluster_val, coord_val, damp_value):\n",
    "    return (dr_val, cluster_val, coord_val, damp_value)\n",
    "\n",
    "\n",
    "# -- Point Selection Mechanics -- #\n",
    "@app.callback(Output(\"table\", \"data\"),\n",
    "              Output(\"3d_scat\", \"figure\"),\n",
    "              Input('3d_scat', 'clickData'),\n",
    "              Input(\"signal\", \"data\"))\n",
    "def select_point(clickData, value):\n",
    "    ctx = dash.callback_context\n",
    "    ids = [c['prop_id'] for c in ctx.triggered]\n",
    "\n",
    "    embeddings = dimension_reduct_store(value[0])\n",
    "    model = clustering_algo_store(value[1], value[3])\n",
    "    named_embeddings = coordinate_space_store(value[2], embeddings)\n",
    "    df = dataframe_store(named_embeddings)\n",
    "\n",
    "    clustering_model = model.fit(named_embeddings[:, 1:4].astype(float))\n",
    "\n",
    "    if '3d_scat.clickData' in ids:\n",
    "        if clickData:\n",
    "            for p in clickData['points']:\n",
    "                if value[1] != \"KNN\":\n",
    "                    return no_update, no_update\n",
    "\n",
    "                coord_list = [p['x'], p['y'], p['z']]\n",
    "                query_arr = np.array(coord_list).reshape(1, -1)\n",
    "\n",
    "                _, neighbors = clustering_model.kneighbors(X=query_arr)\n",
    "                neighbors_list = neighbors.tolist()[0]\n",
    "                tokens = []\n",
    "                for idx in neighbors_list:\n",
    "                    tokens.append(table.iloc[idx])\n",
    "                update = pd.DataFrame(data=tokens)\n",
    "\n",
    "                selected_df = df[df.index.isin(neighbors_list)]\n",
    "                nonselected_df = df.drop(index=neighbors_list)\n",
    "\n",
    "                ff = px.scatter_3d(\n",
    "                    selected_df,\n",
    "                    x='x1',\n",
    "                    y='x2',\n",
    "                    z='x3',\n",
    "                    hover_name='token')\n",
    "\n",
    "                ff = ff.update_traces(marker=scatter_plot_3d_selected_marker)\n",
    "\n",
    "                ff2 = px.scatter_3d(\n",
    "                    nonselected_df,\n",
    "                    x='x1',\n",
    "                    y='x2',\n",
    "                    z='x3',\n",
    "                    hover_name='token')\n",
    "\n",
    "                ff2 = ff2.update_traces(marker=scatter_plot_3d_nonselected_marker)\n",
    "\n",
    "                ff.add_trace(ff2.data[0])\n",
    "                ff['layout']['uirevision'] = 1\n",
    "\n",
    "                return update.to_dict('records'), ff\n",
    "    elif 'signal.data' in ids:\n",
    "        if value[1] != \"KNN\":\n",
    "            y_pred = clustering_model.predict(embeddings)\n",
    "\n",
    "            df.insert(0, \"Label\", y_pred, True)\n",
    "            ff = px.scatter_3d(\n",
    "                df,\n",
    "                x='x1',\n",
    "                y='x2',\n",
    "                z='x3',\n",
    "                color='Label',\n",
    "                hover_name='token')\n",
    "\n",
    "            ff.update_traces(marker=scatter_plot_3d_marker_no_color)\n",
    "\n",
    "            if \"GMM\" not in value[1]:\n",
    "                centers = pd.DataFrame(data=clustering_model.cluster_centers_, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "                ff2 = px.scatter_3d(\n",
    "                    centers,\n",
    "                    x='x1',\n",
    "                    y='x2',\n",
    "                    z='x3')\n",
    "                ff2.update_traces(marker=scatter_plot_3d_marker_cluster_center)\n",
    "\n",
    "                ff.add_trace(ff2.data[0])\n",
    "        else:\n",
    "            ff = px.scatter_3d(\n",
    "                df,\n",
    "                x='x1',\n",
    "                y='x2',\n",
    "                z='x3',\n",
    "                hover_name='token')\n",
    "\n",
    "            ff.update_traces(marker=scatter_plot_3d_marker)\n",
    "\n",
    "        ff['layout']['uirevision'] = 1\n",
    "\n",
    "        return no_update, ff\n",
    "    else:\n",
    "        return no_update, no_update\n",
    "\n",
    "\n",
    "app.run_server(host='0.0.0.0', mode='jupyterlab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZNPHgwy3XXZ"
   },
   "source": [
    "# Transformer Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmiGMB4_-l8x"
   },
   "source": [
    "### Main (Initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "j7hXYiqW3o6M"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/the-optimal-policies/appresults/w2v_weights.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-53c2be5544fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -- Data Batches, Vocab, and Embedding -- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_embedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"results/w2v_weights.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"results/vocab_dict.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatabase_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSOURCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'database/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/env/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/the-optimal-policies/appresults/w2v_weights.joblib'"
     ]
    }
   ],
   "source": [
    "# -- Data Batches, Vocab, and Embedding -- #\n",
    "word_embedding_matrix = joblib.load(SOURCE + \"results/w2v_weights.joblib\")\n",
    "vocabulary = joblib.load(SOURCE + \"results/vocab_dict.joblib\")\n",
    "dataset = database_builder(SOURCE + 'database/')\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "batched_dataset = process_all_batches()\n",
    "\n",
    "# -- Transformer Model -- #\n",
    "optimus_prime = Transformer(\n",
    "    TRANSFORMER_LAYERS,\n",
    "    W2V_EMBED_SIZE,\n",
    "    TRANSFORMER_HEADS,\n",
    "    TRANSFORMER_DFF,\n",
    "    vocab_size,\n",
    "    word_embedding_matrix,\n",
    "    MAX_SEQ_LEN,\n",
    "    DROPOUT_RATE)\n",
    "\n",
    "# -- Labels -- #\n",
    "label_unique = dataset['label'].unique()\n",
    "lbp = LabelEncoder().fit(label_unique)\n",
    "binary_labels = lbp.transform(label_unique)\n",
    "\n",
    "log_labels = {}\n",
    "for idx, label in enumerate(label_unique):\n",
    "    log_labels.update({\n",
    "        label: binary_labels[idx]\n",
    "    })\n",
    "\n",
    "# -- Model Metrics -- #\n",
    "learning_rate = CustomSchedule(W2V_EMBED_SIZE)\n",
    "epoch_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "epoch_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# -- Classification Step Layers -- #\n",
    "add_att_layer = tf.keras.layers.AdditiveAttention()\n",
    "softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "s1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(BATCH_SIZE, activation=ACTIVATION),\n",
    "    tf.keras.layers.Dense(4, activation=ACTIVATION),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "# -- Pipeline Info -- #\n",
    "n_logs = len(dataset.index)\n",
    "#n_iter = n_logs // BATCH_SIZE\n",
    "n_iter = 5\n",
    "remainder = n_logs % BATCH_SIZE\n",
    "attns = []\n",
    "\n",
    "\n",
    "# -- Checkpoints -- #\n",
    "checkpoint_path = SOURCE + \"checkpoints/\"\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), transformer=optimus_prime, optimizer=optimizer)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtCEbgsytePO"
   },
   "outputs": [],
   "source": [
    "def process_all_batches():\n",
    "    batches = []\n",
    "\n",
    "    for idx in range(n_iter + 1):\n",
    "        log_batch, labels = process_batch(dataset, vocabulary, idx, log_labels)\n",
    "\n",
    "        batches.append((log_batch, labels))\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0dMK9zKv13C"
   },
   "outputs": [],
   "source": [
    "    tf.profiler.experimental.stop()\n",
    "    tf.summary.trace_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKEF1GYqtXSv"
   },
   "source": [
    "### Main (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dc-ZONUfLHe"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    start = time.time()\n",
    "    epoch_loss.reset_states()\n",
    "    epoch_accuracy.reset_states()\n",
    "    dataset_iter = iter(batched_dataset)\n",
    "\n",
    "    t = tqdm(range(n_iter), desc=\"Epoch: {:03d}, Loss: {:.3f}, Accuracy: {:.3%}\".format(0, 0, 0), position=0, leave=True)\n",
    "    for _ in t:\n",
    "        batch = next(dataset_iter)\n",
    "        log_batch = batch[0]\n",
    "        labels = batch[1]\n",
    "\n",
    "        # Returns Eager Tensor for Predictions\n",
    "        tf.summary.trace_on()\n",
    "        tf.profiler.experimental.start(logdir)\n",
    "        with writer.as_default():\n",
    "          train_step(log_batch, labels)\n",
    "          # with tf.summary.record_if(True):\n",
    "\n",
    "          tf.summary.trace_export(\n",
    "            name = \"training_trace\",\n",
    "            step=0,\n",
    "            profiler_outdir=logdir\n",
    "          )\n",
    "\n",
    "        tf.profiler.experimental.stop()\n",
    "        tf.summary.trace_off()\n",
    "        \n",
    "        checkpoint.step.assign_add(1)\n",
    "\n",
    "        if int(checkpoint.step) % 10 == 0:\n",
    "            save_path = checkpoint_manager.save()\n",
    "\n",
    "        t.set_description(desc=\"Epoch: {:03d}, Loss: {:.3f}, Accuracy: {:.3%} \".format(epoch,\n",
    "                                                                    epoch_loss.result(),\n",
    "                                                                    epoch_accuracy.result()))\n",
    "        t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hT-fshbn3WUy"
   },
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=([BATCH_SIZE, None]), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=([BATCH_SIZE]), dtype=tf.float32)\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)#, experimental_compile=True)\n",
    "def train_step(log_batch: tf.Tensor, \n",
    "               labels: tf.Tensor):\n",
    "    transformer_input = tf.tuple([\n",
    "        log_batch,  # <tf.Tensor: shape=(batch_size, max_seq_len), dtype=float32>\n",
    "        labels  # <tf.Tensor: shape=(batch_size, num_classes), dtype=float32>\n",
    "    ])\n",
    "    with tf.GradientTape() as tape:\n",
    "        Rs, _ = optimus_prime(transformer_input)\n",
    "        a_s = add_att_layer([Rs, Rs])\n",
    "        y = softmax(a_s * Rs)\n",
    "        print(a_s.shape)\n",
    "        # y = Rs\n",
    "        loss = tf.py_function(loss_function, [labels, y], tf.float32)\n",
    "        pred = s1(y)\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "    # Optimize the model\n",
    "    grads = tape.gradient(loss, optimus_prime.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, optimus_prime.trainable_variables))\n",
    "\n",
    "    acc = accuracy_function(labels, pred)\n",
    "\n",
    "    # Tracking Progress\n",
    "    epoch_loss.update_state(loss)  # Adding Batch Loss\n",
    "    epoch_accuracy.update_state(acc)\n",
    "\n",
    "    # return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2K5ODbmo_hWj"
   },
   "source": [
    "## Metric Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5QbcqbJ_vZH"
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WytrcTtl-4q2"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73Jm1yHR_wou"
   },
   "source": [
    "### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZodkFf-_09c"
   },
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=1))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MubVySBE_wVH"
   },
   "source": [
    "### Custom Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FjRZs2A_286"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model: int, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JtgL-iQkqj2"
   },
   "source": [
    "## Pipeline Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZGIBu-d_AEK"
   },
   "source": [
    "### ProcessBatch (NEEDS UPDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pk9w4xg_3cMD"
   },
   "outputs": [],
   "source": [
    "def process_batch(dataset: pd.DataFrame,\n",
    "                  vocabulary: dict,\n",
    "                  idx: int,\n",
    "                  labels: dict) -> tuple:\n",
    "    logs = np.zeros((BATCH_SIZE, MAX_SEQ_LEN))\n",
    "    y_true = np.empty((BATCH_SIZE,))\n",
    "\n",
    "    start_window = idx * BATCH_SIZE\n",
    "    end_window = (idx + 1) * BATCH_SIZE\n",
    "    for log_idx, log in enumerate(dataset['log'][start_window:end_window]):\n",
    "        for seq_idx, word in enumerate(log.split()):\n",
    "            if seq_idx >= MAX_SEQ_LEN:\n",
    "                break\n",
    "            logs[log_idx, seq_idx] = vocabulary[word] if word in vocabulary.keys() else 0\n",
    "        y_true[log_idx] = labels[dataset['label'][log_idx]]\n",
    "\n",
    "    return tf.convert_to_tensor(logs, dtype=tf.float32), tf.convert_to_tensor(y_true, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-Dg1Sc6_QU2"
   },
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRk21bVC2oWu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 input_vocab_size,\n",
    "                 embedding_matrix,\n",
    "                 max_seq_len,\n",
    "                 rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # self.embedding = tf.keras.layers.Embedding(\n",
    "        #     input_vocab_size,\n",
    "        #     d_model,\n",
    "        #     weights=[embedding_matrix],\n",
    "        #     input_length=max_seq_len,\n",
    "        #     trainable=False)\n",
    "        \n",
    "        self.embedding = EmbeddingLayer(input_vocab_size, d_model, embedding_matrix, max_seq_len)\n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(max_seq_len, d_model)\n",
    "\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "                        num_layers,\n",
    "                        d_model,\n",
    "                        embedding_matrix,\n",
    "                        num_heads,\n",
    "                        dff,\n",
    "                        input_vocab_size,\n",
    "                        max_seq_len,\n",
    "                        rate) for _ in range(3)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, input_tuple: tf.tuple, **kwargs):\n",
    "        log_batch = input_tuple[0]\n",
    "        encoding_padding_mask = None # input_tuple[1]\n",
    "        \n",
    "        embedding_tensor = self.embedding(log_batch) # (batch_size, input_seq_len, d_model)\n",
    "        embedding_tensor = self.pos_encoding(embedding_tensor)\n",
    "        embedding_tensor = self.dropout(embedding_tensor, training=TRAINING)\n",
    "\n",
    "        # Transformer Block #1\n",
    "        # (batch_size, inp_seq_len, d_model), (batch_size, class, inp_seq_len, inp_seq_len)\n",
    "        enc_output, att = self.transformer_blocks[0](embedding_tensor, encoding_padding_mask)\n",
    "\n",
    "        # Transformer Block #2 vv (takes the place of the Decoder)\n",
    "        fin_output, att = self.transformer_blocks[1](enc_output, encoding_padding_mask)\n",
    "\n",
    "        final_output = tf.reduce_mean(fin_output, axis=1)\n",
    "        final_output = tf.expand_dims(final_output, axis=0)\n",
    "\n",
    "        print(final_output.shape)\n",
    "\n",
    "        out, att = self.transformer_blocks[2](final_output, encoding_padding_mask)\n",
    "\n",
    "        seq_representation = tf.reduce_mean(out, axis=1)\n",
    "        return seq_representation, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vf9VgUB-6OD"
   },
   "source": [
    "### EmbeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsMsrJNHstXc"
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_vocab_size, d_model, embedding_matrix, max_seq_len):\n",
    "    self.max_seq_len = max_seq_len\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(\n",
    "      input_vocab_size,\n",
    "      d_model,\n",
    "      weights=[embedding_matrix],\n",
    "      input_length=max_seq_len,\n",
    "      trainable=False)\n",
    "\n",
    "  def call(self, input):\n",
    "    input_sequences = log_tokenizer.texts_to_sequences(input)\n",
    "    \n",
    "    inputs = pad_sequences(input_sequences, maxlen=self.max_seq_len, padding='post')\n",
    "\n",
    "    embedding_tensor = self.embedding(inputs)\n",
    "    embedding_tensor *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return embedding_tensor\n",
    "\n",
    "  # adding embedding and position encoding.\n",
    "  # embedding_tensor = self.embedding(log_batch, training=TRAINING)  # (batch_size, input_seq_len, d_model)\n",
    "  # embedding_tensor *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zzi2tYRAYC1"
   },
   "source": [
    "### PositionalEncodingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfyQUMo4Aarf"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1  # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksz4Lk2K_WV6"
   },
   "source": [
    "### TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77qno34G23WG"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 d_model,\n",
    "                 embedding_matrix,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 input_vocab_size,\n",
    "                 max_seq_len,\n",
    "                 rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        attn_weights = None\n",
    "        for i in range(self.num_layers):\n",
    "            x, attn_weights = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return tf.convert_to_tensor(x), tf.convert_to_tensor(attn_weights)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARsH5IxX_Th0"
   },
   "source": [
    "### EncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSmeFr022zA0"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 dff: int,\n",
    "                 rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_headed_attention = MultiHeadAttention(num_heads=num_heads,\n",
    "                                                         key_dim=d_model // num_heads,\n",
    "                                                         dropout=0.1)\n",
    "\n",
    "        self.feed_forward_network = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation=ACTIVATION),  # (batch_size, seq_len, dff)\n",
    "            tf.keras.layers.Dense(d_model, activation=ACTIVATION)  # (batch_size, seq_len, d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        # (1) - Attention Score\n",
    "        attn_output, attn_weights = self.multi_headed_attention(x,\n",
    "                                                                x,\n",
    "                                                                return_attention_scores=True)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (2) - Add & Normalize\n",
    "        attn_output = self.dropout1(attn_output, training=TRAINING)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (3) - Feed Forward NN\n",
    "        feed_forward_output = self.feed_forward_network(out1)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        # (4) - Add & Normalize\n",
    "        feed_forward_output = self.dropout2(feed_forward_output, training=TRAINING)\n",
    "        out2 = self.layernorm2(out1 + feed_forward_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return tf.convert_to_tensor(out2), tf.convert_to_tensor(attn_weights)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ITSJA5hWASDn",
    "0JtgL-iQkqj2"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "LongRunTransformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
